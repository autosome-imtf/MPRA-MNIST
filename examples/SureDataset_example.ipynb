{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb50d60d-0caf-443a-ad75-6be23f5a9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpramnist\n",
    "from mpramnist.suredataset import SureDataset\n",
    "from mpramnist import transforms as t\n",
    "from mpramnist import target_transforms as t_t\n",
    "\n",
    "import pyfastx\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tltorch import TRL\n",
    "import torchmtl\n",
    "from rotary_embedding_torch import RotaryEmbedding\n",
    "\n",
    "import boda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99495f77-7f7d-4123-978b-024179fbab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 20\n",
    "NUM_WORKERS = 103\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef20fb37-a43d-4539-978b-ea02acd2f53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# preprocessing\n",
    "train_transform = t.Compose([\n",
    "    t.Seq2Tensor(),\n",
    "    t.Reverse(0.5),\n",
    "])\n",
    "test_transform = t.Compose([ # трансформы теста слегка другие\n",
    "    t.Seq2Tensor(), \n",
    "])\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (seq, targets) = zip(*batch)\n",
    "\n",
    "    seq = pad_sequence(seq, batch_first=True, padding_value=0.25)\n",
    "\n",
    "    return seq, torch.vstack(targets)\n",
    "\n",
    "task = \"classification\"\n",
    "\n",
    "genome_ids = [\"SuRE42_HG02601\",\n",
    "             \"SuRE43_GM18983\",\n",
    "             \"SuRE44_HG01241\",\n",
    "             \"SuRE45_HG03464\"]\n",
    "\n",
    "# load the data\n",
    "train_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"train\", transform=train_transform)                                                                                              # for needed folds\n",
    "val_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"val\", transform=test_transform) # use \"val\" for default validation set\n",
    "test_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"test\", transform=test_transform) # use \"test\" for default test set\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS, collate_fn=pad_collate)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS, collate_fn=pad_collate)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "006b4835-1faf-4a48-9b77-cb07addbf2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_score, recall_score, precision_recall_curve, auc, average_precision_score, roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def calculate_auroc( y_score, y_true, n_classes):\n",
    "\n",
    "    y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "    \n",
    "    y_true = y_true.cpu().numpy()\n",
    "    \n",
    "    return roc_auc_score(\n",
    "        y_true,\n",
    "        y_score,\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"macro\",\n",
    "    )\n",
    "\n",
    "def calculate_aupr(y_score, y_true, n_classes):\n",
    "    y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "    y_pred = np.argmax(y_score, axis=1)\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    \n",
    "    plt.hist(y_pred, color = 'blue', edgecolor = 'black',\n",
    "     bins = n_classes)\n",
    "    plt.title('Histogram count of label')\n",
    "    plt.xlabel('label')\n",
    "    plt.ylabel('count')\n",
    "    arr = []\n",
    "    for i in range(n_classes):\n",
    "        arr.append(y_pred.tolist().count(i))\n",
    "    print(arr)\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')  \n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"F1-score:\", f1)\n",
    "\n",
    "    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "    pr_auc = average_precision_score(y_true_bin, y_score, average=\"macro\")\n",
    "    return pr_auc\n",
    "    \n",
    "def roc_auc(seq_model, loader, n_classes = 10, figure = False):\n",
    "    \n",
    "    y_preds = trainer.predict(seq_model, dataloaders=loader)\n",
    "    y_preds = torch.concat(y_preds)\n",
    "    \n",
    "    targets = []\n",
    "    for i, batch in tqdm(enumerate(loader)):\n",
    "        x, y = batch\n",
    "        targets.append(y.squeeze().long())\n",
    "    targets = torch.concat(targets)\n",
    "\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "\n",
    "    auroc = calculate_auroc(y_preds[:,0:5], targets[:,0], 5) \n",
    "    aupr = calculate_aupr( y_preds[:,0:5], targets[:,0], 5) \n",
    "    print(\"Test AUROC_K562: %.4f\"%(np.nanmean(auroc)))\n",
    "    print(\"Test AUPR_K562 : %.4f\"%(np.nanmean(aupr)))\n",
    "\n",
    "    auroc = calculate_auroc(y_preds[:,5:10], targets[:,1], 5) \n",
    "    aupr = calculate_aupr( y_preds[:,5:10], targets[:,1], 5) \n",
    "    print(\"Test AUROC_HepG2: %.4f\"%(np.nanmean(auroc)))\n",
    "    print(\"Test AUPR_HepG2 : %.4f\"%(np.nanmean(aupr)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f857d38-8410-4464-be44-d0e9afaa3e02",
   "metadata": {},
   "source": [
    "# LegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0878717-740d-4f5f-af36-682171c2bd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        n = m.kernel_size[0] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2 / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(inp, int(inp // reduction)),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(int(inp // reduction), inp),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "class EffBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, resize_factor, activation, out_ch=None, se_reduction=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim = self.in_ch * self.resize_factor\n",
    "\n",
    "        block = nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=ks,\n",
    "                            groups=self.inner_dim,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       SELayer(self.inner_dim, reduction=self.se_reduction),\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.in_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.in_ch),\n",
    "                       activation(),\n",
    "        )\n",
    "\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.out_ch,\n",
    "                            kernel_size=self.ks,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.out_ch),\n",
    "                       activation()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)\n",
    "\n",
    "class MapperBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.Conv1d(in_channels=in_features,\n",
    "                      out_channels=out_features,\n",
    "                      kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class HumanLegNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch,\n",
    "                 output_dim,\n",
    "                 stem_ch,\n",
    "                 stem_ks,\n",
    "                 ef_ks,\n",
    "                 ef_block_sizes,\n",
    "                 pool_sizes,\n",
    "                 resize_factor,\n",
    "                 activation=nn.SiLU,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert len(pool_sizes) == len(ef_block_sizes)\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.stem = LocalBlock(in_ch=in_ch,\n",
    "                               out_ch=stem_ch,\n",
    "                               ks=stem_ks,\n",
    "                               activation=activation)\n",
    "\n",
    "        blocks = []\n",
    "        self.output_dim = output_dim\n",
    "        in_ch = stem_ch\n",
    "        out_ch = stem_ch\n",
    "        for pool_sz, out_ch in zip(pool_sizes, ef_block_sizes):\n",
    "            blc = nn.Sequential(\n",
    "                ResidualConcat(\n",
    "                    EffBlock(\n",
    "                        in_ch=in_ch,\n",
    "                        out_ch=in_ch,\n",
    "                        ks=ef_ks,\n",
    "                        resize_factor=resize_factor,\n",
    "                        activation=activation)\n",
    "                ),\n",
    "                LocalBlock(in_ch=in_ch * 2,\n",
    "                           out_ch=out_ch,\n",
    "                           ks=ef_ks,\n",
    "                           activation=activation),\n",
    "                nn.MaxPool1d(pool_sz) if pool_sz != 1 else nn.Identity()\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "            blocks.append(blc)\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper = MapperBlock(in_features=out_ch,\n",
    "                                  out_features=out_ch * 2)\n",
    "        self.head = nn.Sequential(nn.Linear(out_ch * 2, out_ch * 2),\n",
    "                                   nn.BatchNorm1d(out_ch * 2),\n",
    "                                   activation(),\n",
    "                                   nn.Linear(out_ch * 2, self.output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.stem(x)\n",
    "        x = self.main(x)\n",
    "        x = self.mapper(x)\n",
    "        x =  F.adaptive_avg_pool1d(x, 1)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0f4005-6ab1-47f6-af50-de29facfdf59",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MPRAnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8607e668-56b4-4d61-84dc-1ff5e09416c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPRAnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=4, out_channels=250, kernel_size=7, padding=\"valid\")\n",
    "        self.bn1 = nn.BatchNorm1d(250)\n",
    "        self.conv2 = nn.Conv1d(in_channels=250, out_channels=250, kernel_size=8, padding=\"valid\")\n",
    "        self.bn2 = nn.BatchNorm1d(250)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=250, out_channels=250, kernel_size=3, padding=\"valid\")\n",
    "        self.bn3 = nn.BatchNorm1d(250)\n",
    "        self.conv4 = nn.Conv1d(in_channels=250, out_channels=100, kernel_size=2, padding=\"valid\")\n",
    "        self.bn4 = nn.BatchNorm1d(100)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=1, stride=1) # does nothing, idk why they even have this layer, but keeping it for consistency\n",
    "        self.dropout2 = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.LazyLinear(300)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(300, 200)\n",
    "\n",
    "        self.embed_dims = 200\n",
    "        self.head = nn.Sequential(nn.Linear(self.embed_dims, 2),\n",
    "                                 )\n",
    "\n",
    "    def forward(self, seq):\n",
    "        #seq = seq.permute(0, 2, 1)\n",
    "        seq = self.conv1(seq)\n",
    "        seq = F.relu(seq)\n",
    "        seq = self.bn1(seq)\n",
    "        seq = self.conv2(seq)\n",
    "        seq = F.softmax(seq, dim=1)\n",
    "        seq = self.bn2(seq)\n",
    "        seq = self.pool1(seq)\n",
    "        seq = self.dropout1(seq)\n",
    "        seq = self.conv3(seq)\n",
    "        seq = F.softmax(seq, dim=1)\n",
    "        seq = self.bn3(seq)\n",
    "        seq = self.conv4(seq)\n",
    "        seq = F.softmax(seq, dim=1)\n",
    "        seq = self.bn4(seq)\n",
    "        seq = self.pool2(seq)\n",
    "        seq = self.dropout2(seq)\n",
    "        seq = seq.reshape((seq.shape[0], -1))\n",
    "        seq = self.fc1(seq)\n",
    "        seq = F.sigmoid(seq)\n",
    "        seq = self.dropout3(seq)\n",
    "        seq = self.fc2(seq)\n",
    "        seq = F.sigmoid(seq)\n",
    "        seq = self.head(seq)\n",
    "\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c5053-6738-40b9-a73f-2af4adc16f88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35ffbc5-a6fe-4e6c-b151-552780e40695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    vx = x - torch.mean(x, dim=0)\n",
    "    vy = y - torch.mean(y, dim=0)\n",
    "    pearsons = torch.sum(vx * vy, dim=0) / (torch.sqrt(torch.sum(vx ** 2, dim=0)) * torch.sqrt(torch.sum(vy ** 2, dim=0)) + 1e-10)\n",
    "    return torch.mean(pearsons)\n",
    "    \n",
    "class Seq1Model(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch = 10, lr=3e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = HumanLegNet(in_ch=in_ch,\n",
    "                                 output_dim = out_ch,\n",
    "                                 stem_ch=64,\n",
    "                                 stem_ks=11,\n",
    "                                 ef_ks=9,\n",
    "                                 ef_block_sizes=[80, 96, 112, 128],\n",
    "                                 pool_sizes=[2,2,2,2],\n",
    "                                 resize_factor=4)\n",
    "        self.model.apply(initialize_weights)\n",
    "        \n",
    "        #self.model = MTLucifer()\n",
    "        model_config = [\n",
    "                                {\n",
    "                                    'name': \"Backbone\",\n",
    "                                    'layers': self.model,\n",
    "                                    # No anchor_layer means this layer receives input directly\n",
    "                                }\n",
    "                            ]\n",
    "        self.n_classes = out_ch\n",
    "        self.loss = torch.nn.CrossEntropyLoss().cuda() # for classification\n",
    "        self.lr = lr\n",
    "        self.val_loss = []\n",
    "        \n",
    "        self.y_score_1 = []\n",
    "        self.y_score_2 = []\n",
    "        self.y_true_1 = []\n",
    "        self.y_true_2 = []\n",
    "\n",
    "        self.train_loss = []\n",
    "        #self.train_pears = []\n",
    "        self.train_auc = []\n",
    "        self.train_aupr = []\n",
    "        self.head = nn.Sequential(nn.Linear(self.model.embed_dims, self.model.embed_dims),\n",
    "                                   nn.BatchNorm1d(self.model.embed_dims),\n",
    "                                   nn.SiLU(),\n",
    "                                   nn.Linear(self.model.embed_dims, out_ch))\n",
    "        model_config.append({\n",
    "                                'name': \"model\",\n",
    "                                'layers': self.head,\n",
    "                                'loss': self.loss,\n",
    "                                'loss_weight': torch.tensor(1.0),\n",
    "                                'anchor_layer': 'Backbone'\n",
    "                            })\n",
    "        self.model = torchmtl.MTLModel(model_config, output_tasks=[\"model\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    '''\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "        y = y.squeeze().long()\n",
    "        #y = self.ohe_y(self.n_classes, y)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=True, on_epoch=True, logger = True)\n",
    "        self.train_loss.append(loss)\n",
    "        \n",
    "        lr = self.optimizers().param_groups[0]['lr']  # Get current learning rate\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    '''\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y.long()\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=True, on_epoch=True, logger = True)\n",
    "        self.train_loss.append(loss)\n",
    "        \n",
    "        lr = self.optimizers().param_groups[0]['lr']  # Get current learning rate\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        y_hat = self.model(x)\n",
    "        y = y.squeeze().long()\n",
    "        \n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.val_yhat.append(y_hat)\n",
    "        self.val_y.append(y)\n",
    "        \n",
    "    '''\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y.long()\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.y_score_1.append(pred[:, 0:5])\n",
    "        self.y_score_2.append(pred[:, 5:10])\n",
    "        self.y_true_1.append(y[:,0])\n",
    "        self.y_true_2.append(y[:,1])\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        if len(self.train_loss) != 0:\n",
    "            val_loss = torch.stack(self.val_loss, dim = 0).mean()\n",
    "            \n",
    "            y_score = torch.concat(self.y_score_1, dim = 0)\n",
    "            y_true = torch.concat(self.y_true_1, dim = 0)\n",
    "            \n",
    "            auroc1 = self.calculate_auroc(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            aupr1 = self.calculate_aupr(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            print(\"roc_auc_k562 \",auroc1)\n",
    "            print(\"aupr_k562\",aupr1)\n",
    "\n",
    "            y_score = torch.concat(self.y_score_2, dim = 0)\n",
    "            y_true = torch.concat(self.y_true_2, dim = 0)\n",
    "            \n",
    "            auroc2 = self.calculate_auroc(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            aupr2 = self.calculate_aupr(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            print(\"roc_auc_heph2 \",auroc2)\n",
    "            print(\"aupr_hepg2\",aupr2)\n",
    "            train_loss = torch.stack(self.train_loss, dim = 0).mean()\n",
    "        \n",
    "            res_str = '|' + ' {}: {:.5f} |'.format(\"current_epoch\", self.current_epoch) \n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_loss\", val_loss)\n",
    "\n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_auc_roc\", np.mean([auroc1, auroc2]))\n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_auc_pr\", np.mean([aupr1,aupr2]))\n",
    "\n",
    "            res_str += ' {}: {:.5f} |'.format(\"train_loss\", train_loss)\n",
    "            \n",
    "            border = '-'*len(res_str)\n",
    "            print(\"\\n\".join(['',border, res_str, border,'']))\n",
    "            self.val_loss.clear()\n",
    "\n",
    "            self.y_score_1.clear()\n",
    "            self.y_score_2.clear()\n",
    "            self.y_true_1.clear()\n",
    "            self.y_true_2.clear()\n",
    "    \n",
    "            self.train_loss.clear()\n",
    "\n",
    "        return None\n",
    "\n",
    "    '''\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        y = y.squeeze().long()\n",
    "        \n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "        '''\n",
    "    def test_step(self, batch, _):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y.long()\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "        \n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "\n",
    "    def calculate_auroc(self, y_score, y_true, n_classes):\n",
    "\n",
    "        y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "        \n",
    "        y_true = y_true.cpu().numpy()\n",
    "        \n",
    "        return roc_auc_score(\n",
    "            y_true,\n",
    "            y_score,\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "    def calculate_aupr(self, y_score, y_true, n_classes):\n",
    "        y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        y_true = y_true.cpu().numpy()\n",
    "    \n",
    "        arr = []\n",
    "        for i in range(n_classes):\n",
    "            arr.append(y_pred.tolist().count(i))\n",
    "        print(arr)\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')  \n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"F1-score:\", f1)\n",
    "\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        pr_auc = average_precision_score(y_true_bin, y_score, average=\"macro\")\n",
    "        return pr_auc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            x, _ = batch\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        #### Malinois\n",
    "        '''\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), \n",
    "                                     lr=self.lr, weight_decay=1e-4, \n",
    "                                     betas=(0.8661062881299633, 0.879223105336538), amsgrad=True)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                         T_0=4096)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "        '''\n",
    "        #### LegNet\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                               lr=self.lr,\n",
    "                                               weight_decay = 1e-2)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, # type: ignore\n",
    "                                                        max_lr=self.lr,\n",
    "                                                        three_phase=False, \n",
    "                                                        total_steps=self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                        pct_start=0.3,\n",
    "                                                        cycle_momentum =False)\n",
    "        lr_scheduler_config = {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                    \"name\": \"cycle_lr\"\n",
    "            }\n",
    "            \n",
    "        return [self.optimizer], [lr_scheduler_config]\n",
    "        '''\n",
    "        self.optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=1e-2)\n",
    "        \n",
    "        return self.optimizer\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f31dc7-5319-4493-83cb-7479a4aab0c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93861757-b996-4f37-be81-4008c6e5ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-02-19 17:09:21.335350: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-19 17:09:21.349452: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739974161.366122 3361863 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739974161.370626 3361863 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 17:09:21.387488: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | MTLModel         | 1.3 M  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "2 | head  | Sequential       | 68.9 K | train\n",
      "---------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.299     Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db3a4a37fe44b0a9aaf306bd72f7357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9983, 3454, 50352, 294, 11900]\n",
      "Precision: 0.25378471566875016\n",
      "Recall: 0.2586692633894424\n",
      "Accuracy: 0.2563731360962321\n",
      "F1-score: 0.19986332451450278\n",
      "roc_auc_k562  0.6105387267744014\n",
      "aupr_k562 0.2763934706772607\n",
      "[31294, 35179, 7966, 21, 1523]\n",
      "Precision: 0.21315558648112196\n",
      "Recall: 0.22503597270610634\n",
      "Accuracy: 0.2362107313478015\n",
      "F1-score: 0.16860526625614405\n",
      "roc_auc_heph2  0.5659496825953891\n",
      "aupr_hepg2 0.23737824844602615\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 2.26152 | val_auc_roc: 0.58824 | val_auc_pr: 0.25689 | train_loss: 2.10907 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27269, 12648, 25476, 247, 8295]\n",
      "Precision: 0.3379959096336961\n",
      "Recall: 0.31960031187728777\n",
      "Accuracy: 0.3210928518293095\n",
      "F1-score: 0.27791541494116406\n",
      "roc_auc_k562  0.6697687980057617\n",
      "aupr_k562 0.32196020845131657\n",
      "[28744, 23478, 7040, 1434, 13239]\n",
      "Precision: 0.29028008606783634\n",
      "Recall: 0.290276542633062\n",
      "Accuracy: 0.29283830391560156\n",
      "F1-score: 0.25086280153883134\n",
      "roc_auc_heph2  0.6303518970635495\n",
      "aupr_hepg2 0.28690995475323117\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | val_loss: 2.11300 | val_auc_roc: 0.65006 | val_auc_pr: 0.30444 | train_loss: 2.08171 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29472, 14187, 23833, 153, 6290]\n",
      "Precision: 0.34324148603343224\n",
      "Recall: 0.32357581253265677\n",
      "Accuracy: 0.3255968080070332\n",
      "F1-score: 0.27862705511316455\n",
      "roc_auc_k562  0.6720500462284104\n",
      "aupr_k562 0.3240105128962372\n",
      "[37462, 20274, 7077, 7, 9115]\n",
      "Precision: 0.3498983667171937\n",
      "Recall: 0.29374795248959173\n",
      "Accuracy: 0.2968282951240955\n",
      "F1-score: 0.23471184494693728\n",
      "roc_auc_heph2  0.6367484286405711\n",
      "aupr_hepg2 0.29033067963194636\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | val_loss: 2.11028 | val_auc_roc: 0.65440 | val_auc_pr: 0.30717 | train_loss: 2.06736 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25179, 15426, 24342, 767, 8221]\n",
      "Precision: 0.3467008972782805\n",
      "Recall: 0.3349064850703783\n",
      "Accuracy: 0.3365118009062014\n",
      "F1-score: 0.29869674794291\n",
      "roc_auc_k562  0.6859438211905703\n",
      "aupr_k562 0.33725227838833594\n",
      "[28663, 16026, 9909, 313, 19024]\n",
      "Precision: 0.299507064645457\n",
      "Recall: 0.3199127852577319\n",
      "Accuracy: 0.3225535943734361\n",
      "F1-score: 0.2746562953627968\n",
      "roc_auc_heph2  0.6498672901721544\n",
      "aupr_hepg2 0.30195639316270767\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 3.00000 | val_loss: 2.08752 | val_auc_roc: 0.66791 | val_auc_pr: 0.31960 | train_loss: 2.05518 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29829, 13543, 23069, 574, 6920]\n",
      "Precision: 0.3566675092685371\n",
      "Recall: 0.3351559170013845\n",
      "Accuracy: 0.3370663420572124\n",
      "F1-score: 0.29215219513802493\n",
      "roc_auc_k562  0.6885551823223555\n",
      "aupr_k562 0.3405192608303834\n",
      "[35929, 13848, 12281, 276, 11601]\n",
      "Precision: 0.30493697900892675\n",
      "Recall: 0.31917106620744157\n",
      "Accuracy: 0.32216135794955025\n",
      "F1-score: 0.26696826164852155\n",
      "roc_auc_heph2  0.6574521665555441\n",
      "aupr_hepg2 0.30842931430628645\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 4.00000 | val_loss: 2.08717 | val_auc_roc: 0.67300 | val_auc_pr: 0.32447 | train_loss: 2.04240 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "seq_model = Seq1Model(in_ch = 4, out_ch = 10, lr = 1e-2)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bfd898-a955-4a95-bd53-3785d14ed4f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-02-19 13:08:45.813385: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-19 13:08:45.827813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739959725.844835 2514784 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739959725.849781 2514784 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-19 13:08:45.866831: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | MTLucifer        | 66.3 M | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "66.3 M    Trainable params\n",
      "320       Non-trainable params\n",
      "66.3 M    Total params\n",
      "265.100   Total estimated model params size (MB)\n",
      "83        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5b39f4fbf843389eece5924272313c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 23334, 6, 72, 0, 397, 0, 0, 10748, 592, 751, 0, 8155, 61, 64, 10859, 5080, 4114, 0, 0, 221, 26, 3175, 3297, 2984]\n",
      "Precision: 0.06686444829671098\n",
      "Recall: 0.07342576795984902\n",
      "Accuracy: 0.07270589189408377\n",
      "F1-score: 0.04170760074942066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  0.6526320869041062\n",
      "aupr  0.06741457009807886\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 3.16944 | val_auc_roc: 0.65263 | val_auc_pr: 0.06741 | train_loss: 3.62026 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4519, 7393, 1187, 20, 9733, 15680, 6, 467, 1230, 204, 5073, 0, 7393, 2474, 8711, 69, 4115, 918, 0, 0, 0, 2064, 2, 2677, 0]\n",
      "Precision: 0.08092348633594464\n",
      "Recall: 0.0846092710051318\n",
      "Accuracy: 0.08476364374112395\n",
      "F1-score: 0.05667022131752008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc  0.6829765581668349\n",
      "aupr  0.07690546428252026\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | val_loss: 3.12505 | val_auc_roc: 0.68298 | val_auc_pr: 0.07691 | train_loss: 2.98506 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de2e005-9b09-41b1-9dc7-35dfd1bb9752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820e8f4e660740218f4424d3b4b089ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mroc_auc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 54\u001b[0m, in \u001b[0;36mroc_auc\u001b[0;34m(seq_model, loader, n_classes, figure)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mroc_auc\u001b[39m(seq_model, loader, n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, figure \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(seq_model, dataloaders\u001b[38;5;241m=\u001b[39mloader)\n\u001b[0;32m---> 54\u001b[0m     y_preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_preds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     targets \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(loader)):\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got tuple"
     ]
    }
   ],
   "source": [
    "roc_auc(seq_model, test_loader, n_classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec5f6084-f1db-4056-b644-8935ed162e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        n = m.kernel_size[0] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2 / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(inp, int(inp // reduction)),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(int(inp // reduction), inp),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "class EffBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, resize_factor, activation, out_ch=None, se_reduction=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim = self.in_ch * self.resize_factor\n",
    "\n",
    "        block = nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=ks,\n",
    "                            groups=self.inner_dim,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       SELayer(self.inner_dim, reduction=self.se_reduction),\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.in_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.in_ch),\n",
    "                       activation(),\n",
    "        )\n",
    "\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.out_ch,\n",
    "                            kernel_size=self.ks,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.out_ch),\n",
    "                       activation()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)\n",
    "\n",
    "class MapperBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.Conv1d(in_channels=in_features,\n",
    "                      out_channels=out_features,\n",
    "                      kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class HumanLegNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch,\n",
    "                 stem_ch,\n",
    "                 stem_ks,\n",
    "                 ef_ks,\n",
    "                 ef_block_sizes,\n",
    "                 pool_sizes,\n",
    "                 resize_factor,\n",
    "                 output_dim,\n",
    "                 activation=nn.SiLU\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert len(pool_sizes) == len(ef_block_sizes)\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.stem = LocalBlock(in_ch=in_ch,\n",
    "                               out_ch=stem_ch,\n",
    "                               ks=stem_ks,\n",
    "                               activation=activation)\n",
    "\n",
    "        blocks = []\n",
    "        self.output_dim = output_dim\n",
    "        in_ch = stem_ch\n",
    "        out_ch = stem_ch\n",
    "        for pool_sz, out_ch in zip(pool_sizes, ef_block_sizes):\n",
    "            blc = nn.Sequential(\n",
    "                ResidualConcat(\n",
    "                    EffBlock(\n",
    "                        in_ch=in_ch,\n",
    "                        out_ch=in_ch,\n",
    "                        ks=ef_ks,\n",
    "                        resize_factor=resize_factor,\n",
    "                        activation=activation)\n",
    "                ),\n",
    "                LocalBlock(in_ch=in_ch * 2,\n",
    "                           out_ch=out_ch,\n",
    "                           ks=ef_ks,\n",
    "                           activation=activation),\n",
    "                nn.MaxPool1d(pool_sz) if pool_sz != 1 else nn.Identity()\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "            blocks.append(blc)\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper = MapperBlock(in_features=out_ch,\n",
    "                                  out_features=out_ch * 2)\n",
    "        self.embed_dims = out_ch * 2\n",
    "        \n",
    "        self.head = nn.Sequential(nn.Linear(out_ch * 2, out_ch * 2),\n",
    "                                   nn.BatchNorm1d(out_ch * 2),\n",
    "                                   activation(),\n",
    "                                   nn.Linear(out_ch * 2, self.output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x= x.permute(0,2,1)\n",
    "        #print(x.size())\n",
    "        x = self.stem(x)\n",
    "        x = self.main(x)\n",
    "        x = self.mapper(x)\n",
    "        x =  F.adaptive_avg_pool1d(x, 1)\n",
    "        x = x.squeeze(-1)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1514943b-e45e-48fb-8cb2-b3cd6a886260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    vx = x - torch.mean(x, dim=0)\n",
    "    vy = y - torch.mean(y, dim=0)\n",
    "    pearsons = torch.sum(vx * vy, dim=0) / (torch.sqrt(torch.sum(vx ** 2, dim=0)) * torch.sqrt(torch.sum(vy ** 2, dim=0)) + 1e-10)\n",
    "    return torch.mean(pearsons)\n",
    "    \n",
    "class Seq1Model(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch = 10, lr=3e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = HumanLegNet(in_ch=in_ch,\n",
    "                                 output_dim = out_ch,\n",
    "                                 stem_ch=64,\n",
    "                                 stem_ks=11,\n",
    "                                 ef_ks=9,\n",
    "                                 ef_block_sizes=[80, 96, 112, 128],\n",
    "                                 pool_sizes=[2,2,2,2],\n",
    "                                 resize_factor=4)\n",
    "        self.model.apply(initialize_weights)\n",
    "        \n",
    "\n",
    "        self.loss = torch.nn.CrossEntropyLoss().cuda() # for classification\n",
    "        self.lr = lr\n",
    "        self.val_loss = []\n",
    "        \n",
    "        self.y_score_1 = []\n",
    "        self.y_score_2 = []\n",
    "        self.y_true_1 = []\n",
    "        self.y_true_2 = []\n",
    "        self.n_classes = out_ch\n",
    "        self.train_loss = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "        y = y.long()\n",
    "\n",
    "        loss = self.loss(y_hat[:, 0:5], y[:,0])\n",
    "        loss += self.loss(y_hat[:, 5:10], y[:,1])\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=True, on_epoch=True, logger = True)\n",
    "        self.train_loss.append(loss)\n",
    "        \n",
    "        lr = self.optimizers().param_groups[0]['lr']  # Get current learning rate\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        y = y.long()\n",
    "\n",
    "        loss = self.loss(y_hat[:, 0:5], y[:,0])\n",
    "        loss += self.loss(y_hat[:, 5:10], y[:,1])\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        self.y_score_1.append(y_hat[:, 0:5])\n",
    "        self.y_score_2.append(y_hat[:, 5:10])\n",
    "        self.y_true_1.append(y[:,0])\n",
    "        self.y_true_2.append(y[:,1])\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        if len(self.train_loss) != 0:\n",
    "            val_loss = torch.stack(self.val_loss, dim = 0).mean()\n",
    "            \n",
    "            y_score = torch.concat(self.y_score_1, dim = 0)\n",
    "            y_true = torch.concat(self.y_true_1, dim = 0)\n",
    "            \n",
    "            auroc1 = self.calculate_auroc(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            aupr1 = self.calculate_aupr(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            print(\"roc_auc_k562 \",auroc1)\n",
    "            print(\"aupr_k562\",aupr1)\n",
    "\n",
    "            y_score = torch.concat(self.y_score_2, dim = 0)\n",
    "            y_true = torch.concat(self.y_true_2, dim = 0)\n",
    "            \n",
    "            auroc2 = self.calculate_auroc(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            aupr2 = self.calculate_aupr(y_score = y_score, y_true = y_true, n_classes = int(self.n_classes/2)) \n",
    "            print(\"roc_auc_heph2 \",auroc2)\n",
    "            print(\"aupr_hepg2\",aupr2)\n",
    "            train_loss = torch.stack(self.train_loss, dim = 0).mean()\n",
    "        \n",
    "            res_str = '|' + ' {}: {:.5f} |'.format(\"current_epoch\", self.current_epoch) \n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_loss\", val_loss)\n",
    "\n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_auc_roc\", np.mean([auroc1, auroc2]))\n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_auc_pr\", np.mean([aupr1,aupr2]))\n",
    "\n",
    "            res_str += ' {}: {:.5f} |'.format(\"train_loss\", train_loss)\n",
    "            \n",
    "            border = '-'*len(res_str)\n",
    "            print(\"\\n\".join(['',border, res_str, border,'']))\n",
    "            self.val_loss.clear()\n",
    "\n",
    "            self.y_score_1.clear()\n",
    "            self.y_score_2.clear()\n",
    "            self.y_true_1.clear()\n",
    "            self.y_true_2.clear()\n",
    "    \n",
    "            self.train_loss.clear()\n",
    "\n",
    "        return None\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        y = y.long()\n",
    "\n",
    "        loss = self.loss(y_hat[:, 0:5], y[:,0])\n",
    "        loss += self.loss(y_hat[:, 5:10], y[:,1])\n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "\n",
    "    def calculate_auroc(self, y_score, y_true, n_classes):\n",
    "\n",
    "        y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "        \n",
    "        y_true = y_true.cpu().numpy()\n",
    "        \n",
    "        return roc_auc_score(\n",
    "            y_true,\n",
    "            y_score,\n",
    "            multi_class=\"ovr\",\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "    def calculate_aupr(self, y_score, y_true, n_classes):\n",
    "        y_score = F.softmax(y_score.float(), dim=1).cpu().numpy()\n",
    "        y_pred = np.argmax(y_score, axis=1)\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        \n",
    "        arr = []\n",
    "        for i in range(n_classes):\n",
    "            arr.append(y_pred.tolist().count(i))\n",
    "        print(arr)\n",
    "        \n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')  \n",
    "        print(\"Precision:\", precision)\n",
    "        print(\"Recall:\", recall)\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        \n",
    "        print(\"Accuracy:\", accuracy)\n",
    "        print(\"F1-score:\", f1)\n",
    "\n",
    "        y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))\n",
    "        pr_auc = average_precision_score(y_true_bin, y_score, average=\"macro\")\n",
    "        return pr_auc\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            x, _ = batch\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        #### Malinois\n",
    "        '''\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, self.parameters()), \n",
    "                                     lr=self.lr, weight_decay=1e-4, \n",
    "                                     betas=(0.8661062881299633, 0.879223105336538), amsgrad=True)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, \n",
    "                                                                         T_0=4096)\n",
    "        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "        return [optimizer], [scheduler]\n",
    "        '''\n",
    "        #### LegNet\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                               lr=self.lr,\n",
    "                                               weight_decay = 1e-2)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, # type: ignore\n",
    "                                                        max_lr=self.lr,\n",
    "                                                        three_phase=False, \n",
    "                                                        total_steps=self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                        pct_start=0.3,\n",
    "                                                        cycle_momentum =False)\n",
    "        lr_scheduler_config = {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                    \"name\": \"cycle_lr\"\n",
    "            }\n",
    "            \n",
    "        return [self.optimizer], [lr_scheduler_config]\n",
    "        '''\n",
    "        self.optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr, weight_decay=1e-2)\n",
    "        \n",
    "        return self.optimizer\n",
    "        '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe492fe-4727-4400-b7d0-e96e21485575",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type             | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | model | HumanLegNet      | 1.3 M  | train\n",
      "1 | loss  | CrossEntropyLoss | 0      | train\n",
      "---------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.299     Total estimated model params size (MB)\n",
      "117       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b944041390494057a407390a13c47c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15851, 26550, 27082, 1872, 4628, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.28337739432390385\n",
      "Recall: 0.2905353641684906\n",
      "Accuracy: 0.2941842254188437\n",
      "F1-score: 0.2571984763553513\n",
      "roc_auc_k562  0.6507437254712176\n",
      "aupr_k562 0.3054984725557134\n",
      "[35828, 17825, 18967, 0, 3363, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.18883582095350615\n",
      "Recall: 0.2594193859136753\n",
      "Accuracy: 0.273337457062764\n",
      "F1-score: 0.20342750505343604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_heph2  0.6132182045366085\n",
      "aupr_hepg2 0.27278590753423493\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 3.05998 | val_auc_roc: 0.63198 | val_auc_pr: 0.28914 | train_loss: 3.00018 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37843, 11495, 18466, 0, 6131, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.2633838327527165\n",
      "Recall: 0.3038402306263016\n",
      "Accuracy: 0.3057280043281261\n",
      "F1-score: 0.2497643248522618\n",
      "roc_auc_k562  0.6611820028214703\n",
      "aupr_k562 0.3136674069790099\n",
      "[43672, 13791, 9457, 0, 7015, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.23133118732456834\n",
      "Recall: 0.2838642655196787\n",
      "Accuracy: 0.28690065598160547\n",
      "F1-score: 0.221320822244635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc_heph2  0.6246537048552774\n",
      "aupr_hepg2 0.28059885363541154\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | val_loss: 3.07242 | val_auc_roc: 0.64292 | val_auc_pr: 0.29713 | train_loss: 2.93753 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31396, 15900, 20403, 270, 5966, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.34364357862521866\n",
      "Recall: 0.3201172752053901\n",
      "Accuracy: 0.32221545952525865\n",
      "F1-score: 0.2742969605746767\n",
      "roc_auc_k562  0.6732755600096897\n",
      "aupr_k562 0.326006278366223\n",
      "[33244, 19195, 10825, 0, 10671, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.24122207223615186\n",
      "Recall: 0.30282476575841866\n",
      "Accuracy: 0.30582268208561575\n",
      "F1-score: 0.2532531530556638\n",
      "roc_auc_heph2  0.6468190635512647\n",
      "aupr_hepg2 0.2994624235697309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | val_loss: 3.00431 | val_auc_roc: 0.66005 | val_auc_pr: 0.31273 | train_loss: 2.90179 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34843, 9268, 21948, 40, 7836, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.34349535654381624\n",
      "Recall: 0.33407040560512347\n",
      "Accuracy: 0.3357814296341381\n",
      "F1-score: 0.2795758436000741\n",
      "roc_auc_k562  0.6851166198219232\n",
      "aupr_k562 0.33786564004278574\n",
      "[37028, 12520, 10343, 55, 13989, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.2886431732640359\n",
      "Recall: 0.3176557332927371\n",
      "Accuracy: 0.3205788868600798\n",
      "F1-score: 0.2602478935776846\n",
      "roc_auc_heph2  0.6515745095554422\n",
      "aupr_hepg2 0.3014313212975178\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 3.00000 | val_loss: 2.97000 | val_auc_roc: 0.66835 | val_auc_pr: 0.31965 | train_loss: 2.86974 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29667, 14073, 22527, 463, 7205, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.3553660291928126\n",
      "Recall: 0.3363592524045703\n",
      "Accuracy: 0.3382295259349429\n",
      "F1-score: 0.2932782564735091\n",
      "roc_auc_k562  0.6881306276346953\n",
      "aupr_k562 0.33976861348385745\n",
      "[36710, 14475, 12169, 305, 10276, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Precision: 0.30384744315827905\n",
      "Recall: 0.31621668133906145\n",
      "Accuracy: 0.3192804490430784\n",
      "F1-score: 0.2626307400262559\n",
      "roc_auc_heph2  0.6575836352548553\n",
      "aupr_hepg2 0.30787583222821435\n",
      "\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 4.00000 | val_loss: 2.95557 | val_auc_roc: 0.67286 | val_auc_pr: 0.32382 | train_loss: 2.83458 |\n",
      "-----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "seq_model = Seq1Model(in_ch = 4, out_ch = 10, lr = 1e-2)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    max_epochs=5,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661bf1a5-6283-41d2-8eab-e4f72fa99278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f459754d1e34f6ebeb000e193549fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [00:01, 53.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29246, 14012, 22093, 476, 8120]\n",
      "Precision: 0.33826702486341265\n",
      "Recall: 0.3268772241655347\n",
      "Accuracy: 0.32837031928272953\n",
      "F1-score: 0.28690354631092785\n",
      "Test AUROC_K562: 0.6812\n",
      "Test AUPR_K562 : 0.3361\n",
      "[35931, 14704, 11360, 345, 11607]\n",
      "Precision: 0.32340299851380594\n",
      "Recall: 0.3281350036833082\n",
      "Accuracy: 0.3310749590923229\n",
      "F1-score: 0.27867898008236647\n",
      "Test AUROC_HepG2: 0.6558\n",
      "Test AUPR_HepG2 : 0.3146\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roc_auc(seq_model, test_loader, n_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8103a7fa-a16a-4185-a590-bb960c061845",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# MTLucifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0286f2-ac8e-41a7-addc-e53596a82e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=\"same\", dilation=1, bias=True, gn_num_groups=None, gn_group_size=16):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Conv1d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, \\\n",
    "                             stride=stride, padding=padding, dilation=dilation, bias=bias)\n",
    "        if gn_num_groups is None:\n",
    "            gn_num_groups = out_channels // gn_group_size\n",
    "        self.gn = nn.GroupNorm(gn_num_groups, out_channels)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        seq = inputs\n",
    "        x = self.gn(F.gelu(self.cnn(seq)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, gn_num_groups=None, gn_group_size=16):\n",
    "        super().__init__()\n",
    "\n",
    "        stride_for_conv1_and_shortcut = 1\n",
    "\n",
    "        if in_channels != out_channels:\n",
    "            stride_for_conv1_and_shortcut = 2\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "\n",
    "        if gn_num_groups is None:\n",
    "            gn_num_groups = out_channels // gn_group_size\n",
    "\n",
    "        # modules for processing the input\n",
    "        self.conv1 = nn.Conv1d(in_channels = in_channels, out_channels = out_channels, kernel_size = kernel_size, stride = stride_for_conv1_and_shortcut, padding = padding, bias=False)\n",
    "        self.gn1 = nn.GroupNorm(gn_num_groups, out_channels)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels = out_channels, out_channels = out_channels, kernel_size = kernel_size, stride = 1, padding = \"same\", bias=False)\n",
    "        self.gn2 = nn.GroupNorm(gn_num_groups, out_channels)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # short cut connections\n",
    "        self.shortcut = nn.Identity()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1, stride = stride_for_conv1_and_shortcut, bias=False)\n",
    "\n",
    "    def forward(self, xl):\n",
    "        input = self.shortcut(xl)\n",
    "\n",
    "        xl = self.relu1(self.gn1(self.conv1(xl)))\n",
    "        xl = self.conv2(xl)\n",
    "\n",
    "        xlp1 = input + xl\n",
    "\n",
    "        xlp1 = self.relu2(self.gn2(xlp1))\n",
    "\n",
    "        return xlp1\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, mlp_dim, dropout=0.1, use_position_embedding=True):\n",
    "        assert d_model % nhead == 0\n",
    "        super().__init__()\n",
    "        embedding_dim = d_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = nhead\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.dropout = dropout\n",
    "        self.use_position_embedding = use_position_embedding\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(self.embedding_dim)\n",
    "        self.xk = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.xq = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.xv = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "        if self.use_position_embedding:\n",
    "            self.rotary_emb = RotaryEmbedding(dim=embedding_dim // self.num_heads)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.embedding_dim)\n",
    "        self.fc2 = nn.Linear(embedding_dim, mlp_dim)\n",
    "        self.fc3 = nn.Linear(mlp_dim, embedding_dim)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        x = self.layer_norm1(inputs)\n",
    "        xk = self.xk(x)\n",
    "        xq = self.xq(x)\n",
    "        xv = self.xv(x)\n",
    "\n",
    "        xk = xk.reshape(xk.shape[0], xk.shape[1], self.num_heads, self.embedding_dim // self.num_heads)\n",
    "        xq = xq.reshape(xq.shape[0], xq.shape[1], self.num_heads, self.embedding_dim // self.num_heads)\n",
    "        xv = xv.reshape(xv.shape[0], xv.shape[1], self.num_heads, self.embedding_dim // self.num_heads)\n",
    "\n",
    "        if self.use_position_embedding:\n",
    "            # make xq and xk have shape (batch_size, num_heads, seq_len, embedding_dim // num_heads)\n",
    "            xq = xq.permute(0, 2, 1, 3)\n",
    "            xk = xk.permute(0, 2, 1, 3)\n",
    "            xq = self.rotary_emb.rotate_queries_or_keys(xq, seq_dim=2)\n",
    "            xk = self.rotary_emb.rotate_queries_or_keys(xk, seq_dim=2)\n",
    "            # make xq and xk have shape (batch_size, seq_len, num_heads, embedding_dim // num_heads)\n",
    "            xq = xq.permute(0, 2, 1, 3)\n",
    "            xk = xk.permute(0, 2, 1, 3)\n",
    "        \n",
    "        attention_weights = einops.einsum(xq, xk, '... q h d, ... k h d -> ... h q k')\n",
    "\n",
    "        attention_weights = attention_weights / np.sqrt(self.embedding_dim // self.num_heads)\n",
    "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
    "        attention_weights = self.dropout1(attention_weights)\n",
    "        attention_output = einops.einsum(attention_weights, xv, '... h q k, ... k h d -> ... q h d')\n",
    "        attention_output = einops.rearrange(attention_output, '... h d -> ... (h d)')\n",
    "        attention_output = self.fc1(attention_output)\n",
    "        attention_output = self.dropout2(attention_output)\n",
    "\n",
    "        mlp_inputs = attention_output + inputs\n",
    "        x = self.layer_norm2(mlp_inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = x + mlp_inputs\n",
    "\n",
    "        return x\n",
    "\n",
    "class MTLucifer(nn.Module):\n",
    "    def __init__(self, nucleotide_embed_dims=1024, nheads=8, mlp_dim_ratio=4):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.cls_token_embedding = nn.Parameter(torch.normal(mean=0.0, std=0.02, size=(1, 1, nucleotide_embed_dims)))\n",
    "        self.embed_dims = nucleotide_embed_dims\n",
    "        self.nheads = nheads\n",
    "        self.mlp_dim = nucleotide_embed_dims * mlp_dim_ratio\n",
    "        \n",
    "        self.promoter_cnn = nn.Sequential(\n",
    "                                            CNNBlock(in_channels = 4, out_channels = 256, kernel_size = 5, stride = 1, bias=True),\n",
    "                                            CNNBlock(in_channels = 256, out_channels = 512, kernel_size = 5, stride = 1, bias=True),\n",
    "                                            CNNBlock(in_channels = 512, out_channels = nucleotide_embed_dims, kernel_size = 5, stride = 1, bias=True)\n",
    "                                         )\n",
    "        self.promoter_transformer = nn.Sequential(\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim)\n",
    "                                        )\n",
    "        \n",
    "        self.head = nn.Sequential(nn.Linear(self.embed_dims, 2),\n",
    "                                 )\n",
    "                                 \n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq = seq.permute(0, 2, 1)\n",
    "        seq = self.promoter_cnn(seq)\n",
    "        seq = seq.permute(0, 2, 1)\n",
    "        seq = torch.hstack([self.cls_token_embedding.expand(seq.shape[0], -1, -1), seq])\n",
    "        outs = self.promoter_transformer(seq)[:, 0]\n",
    "        outs = self.head(outs)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ee3823-f3f2-4059-a02b-a7623a285de9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class MTLuciferWithResidualBlocks(nn.Module):\n",
    "    def __init__(self, nucleotide_embed_dims=1024, nheads=8, mlp_dim_ratio=2):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.cls_token_embedding = nn.Parameter(torch.normal(mean=0.0, std=0.02, size=(1, 1, nucleotide_embed_dims)))\n",
    "        self.embed_dims = nucleotide_embed_dims\n",
    "        self.nheads = nheads\n",
    "        self.mlp_dim = nucleotide_embed_dims * mlp_dim_ratio\n",
    "        \n",
    "        self.promoter_cnn = nn.Sequential(\n",
    "                                            CNNBlock(in_channels = 4, out_channels = 256, kernel_size = 20),\n",
    "                                            ResidualBlock(in_channels = 256, out_channels = 256, kernel_size = 5),\n",
    "                                            ResidualBlock(in_channels = 256, out_channels = 512, kernel_size = 5),\n",
    "                                            ResidualBlock(in_channels = 512, out_channels = nucleotide_embed_dims, kernel_size = 5)\n",
    "                                         )\n",
    "        self.promoter_transformer = nn.Sequential(\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim),\n",
    "                                            TransformerBlock(d_model=nucleotide_embed_dims, nhead=self.nheads, mlp_dim=self.mlp_dim)\n",
    "                                        )\n",
    "        self.head = nn.Sequential(nn.Linear(self.embed_dims, 25),\n",
    "                                 )\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        seq = seq.permute(0, 2, 1)\n",
    "        seq = self.promoter_cnn(seq)\n",
    "        seq = seq.permute(0, 2, 1)\n",
    "        seq = torch.hstack([self.cls_token_embedding.expand(seq.shape[0], -1, -1), seq])\n",
    "        outs = self.promoter_transformer(seq)[:, 0]\n",
    "        outs = self.head(outs)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841eeb43-d812-45a8-87fb-123658f6e7bb",
   "metadata": {},
   "source": [
    "# Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "374697d1-2474-42a5-9240-4ccf33d44245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        n = m.kernel_size[0] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2 / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, inp, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "                nn.Linear(inp, int(inp // reduction)),\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(int(inp // reduction), inp),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, = x.size()\n",
    "        y = x.view(b, c, -1).mean(dim=2)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y\n",
    "\n",
    "class EffBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, resize_factor, activation, out_ch=None, se_reduction=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.resize_factor = resize_factor\n",
    "        self.se_reduction = resize_factor if se_reduction is None else se_reduction\n",
    "        self.ks = ks\n",
    "        self.inner_dim = self.in_ch * self.resize_factor\n",
    "\n",
    "        block = nn.Sequential(\n",
    "                        nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.inner_dim,\n",
    "                            kernel_size=ks,\n",
    "                            groups=self.inner_dim,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.inner_dim),\n",
    "                       activation(),\n",
    "                       SELayer(self.inner_dim, reduction=self.se_reduction),\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.inner_dim,\n",
    "                            out_channels=self.in_ch,\n",
    "                            kernel_size=1,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.in_ch),\n",
    "                       activation(),\n",
    "        )\n",
    "\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class LocalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, ks, activation, out_ch=None):\n",
    "        super().__init__()\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = self.in_ch if out_ch is None else out_ch\n",
    "        self.ks = ks\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "                       nn.Conv1d(\n",
    "                            in_channels=self.in_ch,\n",
    "                            out_channels=self.out_ch,\n",
    "                            kernel_size=self.ks,\n",
    "                            padding='same',\n",
    "                            bias=False\n",
    "                       ),\n",
    "                       nn.BatchNorm1d(self.out_ch),\n",
    "                       activation()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ResidualConcat(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return torch.concat([self.fn(x, **kwargs), x], dim=1)\n",
    "\n",
    "class MapperBlock(nn.Module):\n",
    "    def __init__(self, in_features, out_features, activation=nn.SiLU):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.BatchNorm1d(in_features),\n",
    "            nn.Conv1d(in_channels=in_features,\n",
    "                      out_channels=out_features,\n",
    "                      kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class HumanLegNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_ch,\n",
    "                 output_dim,\n",
    "                 stem_ch,\n",
    "                 stem_ks,\n",
    "                 ef_ks,\n",
    "                 ef_block_sizes,\n",
    "                 pool_sizes,\n",
    "                 resize_factor,\n",
    "                 activation=nn.SiLU,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert len(pool_sizes) == len(ef_block_sizes)\n",
    "\n",
    "        self.in_ch = in_ch\n",
    "        self.stem = LocalBlock(in_ch=in_ch,\n",
    "                               out_ch=stem_ch,\n",
    "                               ks=stem_ks,\n",
    "                               activation=activation)\n",
    "\n",
    "        blocks = []\n",
    "        self.output_dim = output_dim\n",
    "        in_ch = stem_ch\n",
    "        out_ch = stem_ch\n",
    "        for pool_sz, out_ch in zip(pool_sizes, ef_block_sizes):\n",
    "            blc = nn.Sequential(\n",
    "                ResidualConcat(\n",
    "                    EffBlock(\n",
    "                        in_ch=in_ch,\n",
    "                        out_ch=in_ch,\n",
    "                        ks=ef_ks,\n",
    "                        resize_factor=resize_factor,\n",
    "                        activation=activation)\n",
    "                ),\n",
    "                LocalBlock(in_ch=in_ch * 2,\n",
    "                           out_ch=out_ch,\n",
    "                           ks=ef_ks,\n",
    "                           activation=activation),\n",
    "                nn.MaxPool1d(pool_sz) if pool_sz != 1 else nn.Identity()\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "            blocks.append(blc)\n",
    "        self.main = nn.Sequential(*blocks)\n",
    "\n",
    "        self.mapper = MapperBlock(in_features=out_ch,\n",
    "                                  out_features=out_ch * 2)\n",
    "        self.embed_dims =  out_ch * 2\n",
    "        '''\n",
    "        self.head = nn.Sequential(nn.Linear(out_ch * 2, out_ch * 2),\n",
    "                                   nn.BatchNorm1d(out_ch * 2),\n",
    "                                   activation(),\n",
    "                                   nn.Linear(out_ch * 2, self.output_dim))\n",
    "        '''\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.stem(x)\n",
    "        x = self.main(x)\n",
    "        x = self.mapper(x)\n",
    "        x =  F.adaptive_avg_pool1d(x, 1)\n",
    "        x = x.squeeze(-1)\n",
    "        #x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64e7c4f2-6e31-40a6-a383-41a9cf75a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    vx = x - torch.mean(x, dim=0)\n",
    "    vy = y - torch.mean(y, dim=0)\n",
    "    pearsons = torch.sum(vx * vy, dim=0) / (torch.sqrt(torch.sum(vx ** 2, dim=0)) * torch.sqrt(torch.sum(vy ** 2, dim=0)) + 1e-10)\n",
    "    return torch.mean(pearsons)\n",
    "    \n",
    "class Seq1Model(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch = 2, lr=3e-4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = HumanLegNet(in_ch=in_ch,\n",
    "                                 output_dim = out_ch,\n",
    "                                 stem_ch=64,\n",
    "                                 stem_ks=11,\n",
    "                                 ef_ks=9,\n",
    "                                 ef_block_sizes=[80, 96, 112, 128],\n",
    "                                 pool_sizes=[2,2,2,2],\n",
    "                                 resize_factor=4)\n",
    "        self.model.apply(initialize_weights)\n",
    "    \n",
    "        self.loss = nn.MSELoss() \n",
    "        self.lr = lr\n",
    "        self.val_loss = []\n",
    "        self.val_pears = [[] for i in range(out_ch)]\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.train_pears = []\n",
    "        model_config = [\n",
    "                                {\n",
    "                                    'name': \"Backbone\",\n",
    "                                    'layers': self.model,\n",
    "                                    # No anchor_layer means this layer receives input directly\n",
    "                                }\n",
    "                            ]\n",
    "        self.n_classes = out_ch\n",
    "        self.output = out_ch\n",
    "        self.head = nn.Sequential(nn.Linear(self.model.embed_dims, self.model.embed_dims),\n",
    "                                   nn.BatchNorm1d(self.model.embed_dims),\n",
    "                                   nn.SiLU(),\n",
    "                                   nn.Linear(self.model.embed_dims, out_ch))\n",
    "        model_config.append({\n",
    "                                'name': \"model\",\n",
    "                                'layers': self.head,\n",
    "                                'loss': self.loss,\n",
    "                                'loss_weight': torch.tensor(1.0),\n",
    "                                'anchor_layer': 'Backbone'\n",
    "                            })\n",
    "        self.model = torchmtl.MTLModel(model_config, output_tasks=[\"model\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=True, on_epoch=True, logger = True)\n",
    "        self.train_loss.append(loss)\n",
    "\n",
    "        corr = pearson_correlation(pred, y)\n",
    "        self.train_pears.append(corr)\n",
    "        lr = self.optimizers().param_groups[0]['lr']  # Get current learning rate\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_loss.append(loss)\n",
    "        \n",
    "        for i in range(y.shape[-1]):\n",
    "            corr = pearson_correlation(pred[:,i], y[:,i])\n",
    "            self.val_pears[i].append(corr)\n",
    "        self.log(f\"val_pearson\", pearson_correlation(pred, y), on_epoch=True, prog_bar=True, on_step = False)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        val_loss = torch.stack(self.val_loss, dim = 0).mean()\n",
    "        #val_pears = torch.stack(self.val_pears, dim = 0).mean()\n",
    "        val_pears = []\n",
    "        if len(self.train_loss) != 0:\n",
    "            train_loss = torch.stack(self.train_loss, dim = 0).mean()\n",
    "            train_pears = torch.stack(self.train_pears, dim = 0).mean()\n",
    "            \n",
    "        res_str = '|' + ' {}: {:.5f} |'.format(\"current_epoch\", self.current_epoch) \n",
    "        res_str += ' {}: {:.5f} |'.format(\"val_loss\", val_loss)\n",
    "        #res_str += ' {}: {:.5f} |'.format(\"val_pearson\", val_pears)\n",
    "        for i in range(self.output):\n",
    "            res_str += '{}: {:.5f} |'.format(f\"val_p_{i}\", torch.tensor(self.val_pears[i]).mean())\n",
    "            val_pears.append(torch.tensor(self.val_pears[i]).mean())\n",
    "        if len(self.train_loss) != 0:\n",
    "            res_str += '\\n {}: {:.5f} |'.format(\"train_loss\", train_loss)\n",
    "            res_str += ' {}: {:.5f} |'.format(\"train_pearson\", train_pears)\n",
    "            res_str += ' {}: {:.5f} |'.format(\"val_pearson\", torch.tensor(val_pears).mean())\n",
    "            \n",
    "        border = '-'*len(res_str)\n",
    "        print(\"\\n\".join(['', border, res_str, border, '']))\n",
    "\n",
    "        self.val_loss.clear()\n",
    "        #self.val_pears.clear()\n",
    "        self.val_pears = [[] for i in range(self.output)]\n",
    "        if len(self.train_loss) != 0:\n",
    "            self.train_loss.clear()\n",
    "            self.train_pears.clear()\n",
    "        return None\n",
    "\n",
    "    def test_step(self, batch, _):\n",
    "        X, y = batch\n",
    "\n",
    "        l_funcs = None\n",
    "        l_weights = None\n",
    "        \n",
    "        y_hat, l_funcs, l_weights = self(X.squeeze(1))            \n",
    "        pred = y_hat[0]\n",
    "        \n",
    "        y = y\n",
    "\n",
    "        std = torch.exp(l_weights[0])**(1/2)\n",
    "        is_regression = int(task == \"regression\")\n",
    "        coeff = 1 / ((is_regression+1)*(std**2))\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            s = 0\n",
    "            loss = 0\n",
    "            output_names = [\"K562_bin\", \"HepG2_bin\"]\n",
    "            num_classes_per_output = [5, 5]\n",
    "            for j, output in enumerate(output_names):\n",
    "                num_outputs_for_this = num_classes_per_output[j]\n",
    "                loss += coeff * l_funcs[0](pred[:, s:s+num_outputs_for_this], y[:, j]) + torch.log(std)\n",
    "                s += num_outputs_for_this\n",
    "        else:\n",
    "            loss = coeff * l_funcs[0](pred, y) + torch.log(std)\n",
    "        \n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "        corr = pearson_correlation(pred, y)\n",
    "        self.log(\"test_pearson\", \n",
    "                 corr ,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 on_step=False,)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            x, _ = batch\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        #### LegNet\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                               lr=self.lr,\n",
    "                                               weight_decay = 1e-2)\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, # type: ignore\n",
    "                                                        max_lr=self.lr,\n",
    "                                                        three_phase=False, \n",
    "                                                        total_steps=self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                        pct_start=0.3,\n",
    "                                                        cycle_momentum =False)\n",
    "        lr_scheduler_config = {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                    \"name\": \"cycle_lr\"\n",
    "            }\n",
    "            \n",
    "        return [self.optimizer], [lr_scheduler_config]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc91ddc-e896-482b-812f-eb829fd17624",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_transform = t_t.Compose([])\n",
    "task = \"regression\"\n",
    "train_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"train\", transform=train_transform, target_transform = target_transform)                                                               # for needed folds\n",
    "val_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"val\", transform=test_transform, target_transform = target_transform) # use \"val\" for default validation set\n",
    "test_dataset = SureDataset(task = task, genome_id = genome_ids[0], split=\"test\", transform=test_transform, target_transform = target_transform) # use \"test\" for default test set\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS, collate_fn=pad_collate)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS, collate_fn=pad_collate)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e244db33-fb9e-42a0-86d1-7caccdeecb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset SureDataset of size 619051 (MpraDaraset)\n",
      "    Number of datapoints: 619051\n",
      "    Default split folds: {}\n",
      "    Used split fold: train\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['SuRE42_HG02601', 'SuRE43_GM18983', 'SuRE44_HG01241', 'SuRE45_HG03464']\n",
      "    Сell type used: SuRE42_HG02601\n",
      "    Target columns that can be used: {}\n",
      "    Sequence size: 196\n",
      "    Number of samples: {}\n",
      "#############################\n",
      "Dataset SureDataset of size 73935 (MpraDaraset)\n",
      "    Number of datapoints: 73935\n",
      "    Default split folds: {}\n",
      "    Used split fold: val\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['SuRE42_HG02601', 'SuRE43_GM18983', 'SuRE44_HG01241', 'SuRE45_HG03464']\n",
      "    Сell type used: SuRE42_HG02601\n",
      "    Target columns that can be used: {}\n",
      "    Sequence size: 361\n",
      "    Number of samples: {}\n",
      "#############################\n",
      "Dataset SureDataset of size 73947 (MpraDaraset)\n",
      "    Number of datapoints: 73947\n",
      "    Default split folds: {}\n",
      "    Used split fold: test\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['SuRE42_HG02601', 'SuRE43_GM18983', 'SuRE44_HG01241', 'SuRE45_HG03464']\n",
      "    Сell type used: SuRE42_HG02601\n",
      "    Target columns that can be used: {}\n",
      "    Sequence size: 298\n",
      "    Number of samples: {}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"#############################\")\n",
    "print(val_dataset)\n",
    "print(\"#############################\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9a9d7c0-bea4-4ae5-b355-6f25174a6cfa",
   "metadata": {},
   "source": [
    "Legnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29251700-96c2-445c-a35c-1c7c715f919a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-03-06 17:50:00.077106: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 17:50:00.186706: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741272600.264905 2539302 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741272600.269704 2539302 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 17:50:00.454052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | HumanLegNet | 1.3 M  | train\n",
      "1 | loss  | MSELoss     | 0      | train\n",
      "----------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.291     Total estimated model params size (MB)\n",
      "117       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 0.00000 |val_p_0: 0.00000 |val_p_1: 0.00000 |\n",
      "----------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd8ce1934d1457786991584e2dbb1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 637.44934 |val_p_0: 0.06495 |val_p_1: 0.02813 |\n",
      " train_loss: 582.86108 | train_pearson: 0.23312 | val_pearson: 0.04654 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7128cfbb1b6d4b0c94338c1ff5858b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                        | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     662.7203369140625     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_pearson        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.04508490115404129    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    662.7203369140625    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.04508490115404129   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 662.7203369140625, 'test_pearson': 0.04508490115404129}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = Seq1Model(in_ch = 4, out_ch = 2, lr = 1e-5)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    max_epochs=1,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc50701e-cb7b-474d-9936-c295eebecbfe",
   "metadata": {},
   "source": [
    "MTlucifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "304da0de-d8f3-4da4-9779-9523a49daf1e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | MTLucifer | 66.3 M | train\n",
      "1 | loss  | MSELoss   | 0      | train\n",
      "--------------------------------------------\n",
      "66.3 M    Trainable params\n",
      "320       Non-trainable params\n",
      "66.3 M    Total params\n",
      "265.005   Total estimated model params size (MB)\n",
      "83        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 0.10449 |val_p_0: 0.00000 |val_p_1: 0.00000 |\n",
      "----------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a96148e57743e085f857e3badd3fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 474.64417 |val_p_0: 0.09712 |val_p_1: 0.05490 |\n",
      " train_loss: 423.12463 | train_pearson: 0.31842 | val_pearson: 0.07601 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ebe53654c9741e881dd51c65e9ae3b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                        | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     495.1138610839844     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_pearson        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.07997729629278183    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    495.1138610839844    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.07997729629278183   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 495.1138610839844, 'test_pearson': 0.07997729629278183}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = Seq1Model(in_ch = 4, out_ch = 2, lr = 1e-5)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    max_epochs=1,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a7253b9-d77f-4f4b-8b25-a5dabd6808a4",
   "metadata": {},
   "source": [
    "Legnet with model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0dbb17-5631-4013-b295-99b03288c0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "2025-03-06 20:58:13.295106: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 20:58:13.308453: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741283893.323813 2957184 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741283893.328315 2957184 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 20:58:13.344983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type       | Params | Mode \n",
      "---------------------------------------------\n",
      "0 | model | MTLModel   | 1.3 M  | train\n",
      "1 | loss  | MSELoss    | 0      | train\n",
      "2 | head  | Sequential | 66.8 K | train\n",
      "---------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.291     Total estimated model params size (MB)\n",
      "118       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 0.50031 |val_p_0: 0.00000 |val_p_1: 0.00000 |\n",
      "----------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552caeea9c3246ab8518c3cd59adb46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                                                                     | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 111.68809 |val_p_0: 0.07253 |val_p_1: 0.03070 |\n",
      " train_loss: 105.24991 | train_pearson: 0.24305 | val_pearson: 0.05162 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47de8b9ef224dffa0b0b95a080e051e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                        | 0/? [00:00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    115.96904754638672     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_pearson        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   0.051520854234695435    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   115.96904754638672    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_pearson       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  0.051520854234695435   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 115.96904754638672, 'test_pearson': 0.051520854234695435}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = Seq1Model(in_ch = 4, out_ch = 2, lr = 1e-5)\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    "    max_epochs=1,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = True,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab531967-346c-468d-a98a-04712c0e4a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mpra]",
   "language": "python",
   "name": "conda-env-mpra-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
