{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9a8e6a-f156-47cd-ac97-5e2859b41671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import mpramnist\n",
    "from mpramnist.malinoisdataset import MalinoisDataset\n",
    "\n",
    "import mpramnist.transforms as t\n",
    "import mpramnist.target_transforms as t_t\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchsummary import summary\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import loggers as pl_loggers\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "import tempfile\n",
    "import hypertune\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b4112e-6aa2-46cf-931c-3b514a1764a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_flank = MalinoisDataset.LEFT_FLANK\n",
    "right_flank = MalinoisDataset.RIGHT_FLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557a30c7-88c1-4384-b2f8-a5b1dd6950f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1076\n",
    "lr = 0.00326\n",
    "NUM_WORKERS = 8\n",
    "weight_d = 0.00034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bb49ad-0ba2-48f5-90b0-826581518c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a55df33-548e-456c-80a4-8275a6e1842a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def shannon_entropy(x):\n",
    "    p_c = nn.Softmax(dim=1)(x)    \n",
    "    return torch.sum(- p_c * torch.log(p_c), axis=1)\n",
    "def pearson_correlation(x, y):\n",
    "    vx = x - torch.mean(x, dim=0)\n",
    "    vy = y - torch.mean(y, dim=0)\n",
    "    pearsons = torch.sum(vx * vy, dim=0) / (torch.sqrt(torch.sum(vx ** 2, dim=0)) * torch.sqrt(torch.sum(vy ** 2, dim=0)) + 1e-10)\n",
    "    return pearsons, torch.mean(pearsons)\n",
    "def _get_ranks(x):\n",
    "    tmp = x.argsort(dim=0)\n",
    "    ranks = torch.zeros_like(tmp)\n",
    "    if len(x.shape) > 1:\n",
    "        dims = x.shape[1]\n",
    "        for dim in range(dims):\n",
    "            ranks[tmp[:,dim], dim] = torch.arange(x.shape[0], layout=x.layout, device=x.device)\n",
    "    else:\n",
    "        ranks[tmp] = torch.arange(x.shape[0], layout=x.layout, device=x.device)\n",
    "    return ranks\n",
    "def spearman_correlation(x, y):\n",
    "    x_rank = _get_ranks(x).float()\n",
    "    y_rank = _get_ranks(y).float()\n",
    "    return pearson_correlation(x_rank, y_rank)\n",
    "\n",
    "def filter_state_dict(model, stashed_dict):\n",
    "    results_dict = { \n",
    "        'filtered_state_dict': {},\n",
    "        'passed_keys'  : [],\n",
    "        'removed_keys' : [],\n",
    "        'missing_keys' : [],\n",
    "        'unloaded_keys': []\n",
    "                   }\n",
    "    old_dict = model.state_dict()\n",
    "\n",
    "    for m_key, m_value in old_dict.items():\n",
    "        try:\n",
    "            \n",
    "            if old_dict[m_key].shape == stashed_dict[m_key].shape:\n",
    "                results_dict['filtered_state_dict'][m_key] = stashed_dict[m_key]\n",
    "                results_dict['passed_keys'].append(m_key)\n",
    "                print(f'Key {m_key} successfully matched', file=sys.stderr)\n",
    "                \n",
    "            else:\n",
    "                check_str = 'Size mismatch for key: {}, expected size {}, got {}' \\\n",
    "                              .format(m_key, old_dict[m_key].shape, stashed_dict[m_key].shape)\n",
    "                results_dict['removed_keys'].append(m_key)\n",
    "                print(check_str, file=sys.stderr)\n",
    "                \n",
    "        except KeyError:\n",
    "            results_dict['missing_keys'].append(m_key)\n",
    "            print(f'Missing key in dict: {m_key}', file=sys.stderr)\n",
    "            \n",
    "    for m_key, m_value in stashed_dict.items():\n",
    "        if m_key not in old_dict.keys():\n",
    "            check_str = 'Skipped loading key: {} of size {}' \\\n",
    "                           .format(m_key, m_value.shape)\n",
    "            results_dict['unloaded_keys'].append(m_key)\n",
    "            print(check_str, file=sys.stderr)\n",
    "            \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45620d1c-a03c-4fa2-a158-a3678a48a4bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class L1KLmixed(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom loss module that combines L1 loss with Kullback-Leibler (KL) divergence loss.\n",
    "\n",
    "    Args:\n",
    "        reduction (str, optional): Specifies the reduction to apply to the losses. Default is 'mean'.\n",
    "        alpha (float, optional): Scaling factor for the L1 loss term. Default is 1.0.\n",
    "        beta (float, optional): Scaling factor for the KL divergence loss term. Default is 1.0.\n",
    "\n",
    "    Attributes:\n",
    "        reduction (str): The reduction method applied to the losses.\n",
    "        alpha (float): Scaling factor for the L1 loss term.\n",
    "        beta (float): Scaling factor for the KL divergence loss term.\n",
    "        MSE (nn.L1Loss): The L1 loss function.\n",
    "        KL (nn.KLDivLoss): The Kullback-Leibler divergence loss function.\n",
    "\n",
    "    Methods:\n",
    "        forward(preds, targets):\n",
    "            Calculate the combined loss by combining L1 and KL divergence losses.\n",
    "\n",
    "    Example:\n",
    "        loss_fn = L1KLmixed()\n",
    "        loss = loss_fn(predictions, targets)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reduction='mean', alpha=1.0, beta=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        \n",
    "        self.MSE = nn.L1Loss(reduction=reduction.replace('batch',''))\n",
    "        self.KL  = nn.KLDivLoss(reduction=reduction, log_target=True)\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        preds_log_prob  = preds   - torch.logsumexp(preds, dim=-1, keepdim=True)\n",
    "        target_log_prob = targets - torch.logsumexp(targets, dim=-1, keepdim=True)\n",
    "        \n",
    "        MSE_loss = self.MSE(preds, targets)\n",
    "        KL_loss  = self.KL(preds_log_prob, target_log_prob)\n",
    "        \n",
    "        combined_loss = MSE_loss.mul(self.alpha) + \\\n",
    "                        KL_loss.mul(self.beta)\n",
    "        \n",
    "        return combined_loss.div(self.alpha+self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d3d2f11-1b42-43df-9ef2-cb6278a0edc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BranchedLinear(nn.Module):\n",
    "    def __init__(self, in_features, hidden_group_size, out_group_size, \n",
    "                 n_branches=1, n_layers=1, \n",
    "                 activation='ReLU', dropout_p=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.hidden_group_size = hidden_group_size\n",
    "        self.out_group_size = out_group_size\n",
    "        self.n_branches = n_branches\n",
    "        self.n_layers   = n_layers\n",
    "        \n",
    "        self.branches = OrderedDict()\n",
    "        \n",
    "        self.nonlin  = getattr(nn, activation)()                               \n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        \n",
    "        self.intake = RepeatLayer(1, n_branches)\n",
    "        cur_size = in_features\n",
    "        \n",
    "        for i in range(n_layers):\n",
    "            if i + 1 == n_layers:\n",
    "                setattr(self, f'branched_layer_{i+1}',  GroupedLinear(cur_size, out_group_size, n_branches))\n",
    "            else:\n",
    "                setattr(self, f'branched_layer_{i+1}',  GroupedLinear(cur_size, hidden_group_size, n_branches))\n",
    "            cur_size = hidden_group_size\n",
    "            \n",
    "    def forward(self, x):\n",
    "\n",
    "        hook = self.intake(x)\n",
    "        \n",
    "        i = -1\n",
    "        for i in range(self.n_layers-1):\n",
    "            hook = getattr(self, f'branched_layer_{i+1}')(hook)\n",
    "            hook = self.dropout( self.nonlin(hook) )\n",
    "        hook = getattr(self, f'branched_layer_{i+2}')(hook)\n",
    "            \n",
    "        return hook\n",
    "\n",
    "class RepeatLayer(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.repeat(*self.args)\n",
    "        \n",
    "class GroupedLinear(nn.Module):\n",
    "    def __init__(self, in_group_size, out_group_size, groups):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_group_size = in_group_size\n",
    "        self.out_group_size= out_group_size\n",
    "        self.groups        = groups\n",
    "        \n",
    "        #initialize weights\n",
    "        self.weight = torch.nn.Parameter(torch.zeros(groups, in_group_size, out_group_size))\n",
    "        self.bias   = torch.nn.Parameter(torch.zeros(groups, 1, out_group_size))\n",
    "        \n",
    "        #change weights to kaiming\n",
    "        self.reset_parameters(self.weight, self.bias)\n",
    "        \n",
    "    def reset_parameters(self, weights, bias):\n",
    "        torch.nn.init.kaiming_uniform_(weights, a=math.sqrt(3))\n",
    "        fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(weights)\n",
    "        bound = 1 / math.sqrt(fan_in)\n",
    "        torch.nn.init.uniform_(bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        reorg = x.permute(1,0).reshape(self.groups, self.in_group_size, -1).permute(0,2,1)\n",
    "        hook  = torch.bmm(reorg, self.weight) + self.bias\n",
    "        reorg = hook.permute(0,2,1).reshape(self.out_group_size*self.groups,-1).permute(1,0)\n",
    "        \n",
    "        return reorg\n",
    "class LinearNorm(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, \n",
    "                 batch_norm=True, weight_norm=True):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear  = nn.Linear(in_features, out_features, bias=True)\n",
    "        if weight_norm:\n",
    "            self.linear = nn.utils.weight_norm(self.linear)\n",
    "        if batch_norm:\n",
    "            self.bn_layer = nn.BatchNorm1d(out_features, eps=1e-05, momentum=0.1, \n",
    "                                           affine=True, track_running_stats=True)\n",
    "    def forward(self, input):\n",
    "        try:\n",
    "            return self.bn_layer( self.linear( input ) )\n",
    "        except AttributeError:\n",
    "            return self.linear( input )\n",
    "class Conv1dNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, \n",
    "                 stride=1, padding=0, dilation=1, groups=1, \n",
    "                 bias=True, batch_norm=True, weight_norm=True):\n",
    "        super(Conv1dNorm, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, \n",
    "                              stride, padding, dilation, groups, bias)\n",
    "        if weight_norm:\n",
    "            self.conv = nn.utils.weight_norm(self.conv)\n",
    "        if batch_norm:\n",
    "            self.bn_layer = nn.BatchNorm1d(out_channels, eps=1e-05, momentum=0.1, \n",
    "                                           affine=True, track_running_stats=True)\n",
    "    def forward(self, input):\n",
    "        try:\n",
    "            return self.bn_layer( self.conv( input ) )\n",
    "        except AttributeError:\n",
    "            return self.conv( input )\n",
    "\n",
    "def get_padding(kernel_size):\n",
    "    \"\"\"\n",
    "    Calculate padding values for convolutional layers.\n",
    "\n",
    "    Args:\n",
    "        kernel_size (int): Size of the convolutional kernel.\n",
    "\n",
    "    Returns:\n",
    "        list: Padding values for left and right sides of the kernel.\n",
    "    \"\"\"\n",
    "    left = (kernel_size - 1) // 2\n",
    "    right= kernel_size - 1 - left\n",
    "    return [ max(0,x) for x in [left,right] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc474511-e82b-4055-a4cb-abea470acb61",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BassetBranched(pl.LightningModule):\n",
    "    ######################\n",
    "    # Model construction #\n",
    "    ######################\n",
    "    \n",
    "    def __init__(self, input_len=600,\n",
    "                 conv1_channels=300, conv1_kernel_size=19, \n",
    "                 conv2_channels=200, conv2_kernel_size=11, \n",
    "                 conv3_channels=200, conv3_kernel_size=7, \n",
    "                 n_linear_layers=2, linear_channels=1000, \n",
    "                 linear_activation='ReLU', linear_dropout_p=0.3, \n",
    "                 n_branched_layers=1, branched_channels=250, \n",
    "                 branched_activation='ReLU6', branched_dropout_p=0., \n",
    "                 n_outputs=280,\n",
    "                 use_batch_norm=True, use_weight_norm=False, \n",
    "                 loss_criterion='L1KLmixed', loss_args={}):                                              \n",
    "        super().__init__()        \n",
    "        \n",
    "        self.input_len         = input_len\n",
    "        \n",
    "        self.conv1_channels    = conv1_channels\n",
    "        self.conv1_kernel_size = conv1_kernel_size\n",
    "        self.conv1_pad = get_padding(conv1_kernel_size)\n",
    "        \n",
    "        self.conv2_channels    = conv2_channels\n",
    "        self.conv2_kernel_size = conv2_kernel_size\n",
    "        self.conv2_pad = get_padding(conv2_kernel_size)\n",
    "\n",
    "        \n",
    "        self.conv3_channels    = conv3_channels\n",
    "        self.conv3_kernel_size = conv3_kernel_size\n",
    "        self.conv3_pad = get_padding(conv3_kernel_size)\n",
    "        \n",
    "        self.n_linear_layers   = n_linear_layers\n",
    "        self.linear_channels   = linear_channels\n",
    "        self.linear_activation = linear_activation\n",
    "        self.linear_dropout_p  = linear_dropout_p\n",
    "        \n",
    "        self.n_branched_layers = n_branched_layers\n",
    "        self.branched_channels = branched_channels\n",
    "        self.branched_activation = branched_activation\n",
    "        self.branched_dropout_p= branched_dropout_p\n",
    "        \n",
    "        self.n_outputs         = n_outputs\n",
    "        \n",
    "        self.loss_criterion    = loss_criterion\n",
    "        self.loss_args         = loss_args\n",
    "        \n",
    "        self.use_batch_norm    = use_batch_norm\n",
    "        self.use_weight_norm   = use_weight_norm\n",
    "        \n",
    "        self.pad1  = nn.ConstantPad1d(self.conv1_pad, 0.)\n",
    "        self.conv1 = Conv1dNorm(4, \n",
    "                                self.conv1_channels, self.conv1_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        self.pad2  = nn.ConstantPad1d(self.conv2_pad, 0.)\n",
    "        self.conv2 = Conv1dNorm(self.conv1_channels, \n",
    "                                self.conv2_channels, self.conv2_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        self.pad3  = nn.ConstantPad1d(self.conv3_pad, 0.)\n",
    "        self.conv3 = Conv1dNorm(self.conv2_channels, \n",
    "                                self.conv3_channels, self.conv3_kernel_size, \n",
    "                                stride=1, padding=0, dilation=1, groups=1, \n",
    "                                bias=True, \n",
    "                                batch_norm=self.use_batch_norm, \n",
    "                                weight_norm=self.use_weight_norm)\n",
    "        \n",
    "        self.pad4 = nn.ConstantPad1d((1,1), 0.)\n",
    "\n",
    "        self.maxpool_3 = nn.MaxPool1d(3, padding=0)\n",
    "        self.maxpool_4 = nn.MaxPool1d(4, padding=0)\n",
    "        \n",
    "        next_in_channels = self.conv3_channels * self.get_flatten_factor(self.input_len)\n",
    "        \n",
    "        for i in range(self.n_linear_layers):\n",
    "            \n",
    "            setattr(self, f'linear{i+1}', \n",
    "                    LinearNorm(next_in_channels, self.linear_channels, \n",
    "                               bias=True, \n",
    "                               batch_norm=self.use_batch_norm, \n",
    "                               weight_norm=self.use_weight_norm)\n",
    "                   )\n",
    "            next_in_channels = self.linear_channels\n",
    "\n",
    "        self.branched = BranchedLinear(next_in_channels, self.branched_channels, \n",
    "                                       self.branched_channels, \n",
    "                                       self.n_outputs, self.n_branched_layers, \n",
    "                                       self.branched_activation, self.branched_dropout_p)\n",
    "            \n",
    "        self.output  = GroupedLinear(self.branched_channels, 1, self.n_outputs)\n",
    "        \n",
    "        self.nonlin  = getattr(nn, self.linear_activation)()                               \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.linear_dropout_p)\n",
    "        \n",
    "        self.criterion = self.loss_criterion\n",
    "    \n",
    "    def get_flatten_factor(self, input_len):\n",
    "        \n",
    "        \n",
    "        \n",
    "        hook = input_len\n",
    "        assert hook % 3 == 0\n",
    "        hook = hook // 3\n",
    "        assert hook % 4 == 0\n",
    "        hook = hook // 4\n",
    "        assert (hook + 2) % 4 == 0\n",
    "        \n",
    "        return (hook + 2) // 4\n",
    "    \n",
    "    ######################\n",
    "    # Model computations #\n",
    "    ######################\n",
    "    \n",
    "    def encode(self, x):\n",
    "        hook = self.nonlin( self.conv1( self.pad1( x ) ) )\n",
    "        hook = self.maxpool_3( hook )\n",
    "        hook = self.nonlin( self.conv2( self.pad2( hook ) ) )\n",
    "        hook = self.maxpool_4( hook )\n",
    "        hook = self.nonlin( self.conv3( self.pad3( hook ) ) )\n",
    "        hook = self.maxpool_4( self.pad4( hook ) )        \n",
    "        hook = torch.flatten( hook, start_dim=1 )\n",
    "        return hook\n",
    "    \n",
    "    def decode(self, x):\n",
    "        hook = x\n",
    "        for i in range(self.n_linear_layers):\n",
    "            hook = self.dropout( \n",
    "                self.nonlin( \n",
    "                    getattr(self,f'linear{i+1}')(hook)\n",
    "                )\n",
    "            )\n",
    "        hook = self.branched(hook)\n",
    "\n",
    "        return hook\n",
    "    \n",
    "    def classify(self, x):\n",
    "        output = self.output( x )\n",
    "        return output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        output  = self.classify(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cafaaa82-c677-47d3-ae88-d1b410ea196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNBasicTraining(pl.LightningModule):\n",
    "\n",
    "    ####################\n",
    "    # Standard methods #\n",
    "    ####################\n",
    "    \n",
    "    def __init__(self, model, optimizer='Adam', amsgrad = True,\n",
    "                          lr = 0.0032658700881052086,\n",
    "                          eps = 1e-08,\n",
    "                          weight_decay = 0.0003438210249762151,\n",
    "                          beta1 = 0.8661062881299633,\n",
    "                          beta2 = 0.879223105336538, scheduler=None, \n",
    "                 scheduler_monitor=None, scheduler_interval='epoch', \n",
    "                 #optimizer_args=None, scheduler_args=None,\n",
    "                T_0=4096, T_mult=1, eta_min=0.0, last_epoch=-1):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = model.criterion\n",
    "        self.amsgrad = amsgrad\n",
    "        self.lr = lr\n",
    "        self.eps = eps\n",
    "        self.weight_decay = weight_decay\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.T_0 = T_0 \n",
    "        self.T_mult = T_mult\n",
    "        self.eta_min = eta_min\n",
    "        self.last_epoch = last_epoch\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.scheduler_monitor = scheduler_monitor\n",
    "        self.scheduler_interval= scheduler_interval\n",
    "        #self.optimizer_args = optimizer_args\n",
    "        #self.scheduler_args = scheduler_args\n",
    "\n",
    "        # 1. Create a list to hold the outputs of `*_step`\n",
    "        self.arit_means = []\n",
    "        self.harm_means = []\n",
    "        self.epoch_pred = []\n",
    "        self.epoch_label = []\n",
    "        self.pearsons = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        return self.model(input)\n",
    "\n",
    "    ###################\n",
    "    # Non-PTL methods #\n",
    "    ###################\n",
    "        \n",
    "    def categorical_mse(self, x, y):\n",
    "        return (x - y).pow(2).mean(dim=0)\n",
    "        \n",
    "    def aug_log(self, internal_metrics=None, external_metrics=None):\n",
    "        if internal_metrics is not None:\n",
    "            for my_key, my_value in internal_metrics.items():\n",
    "                self.log(my_key, my_value)\n",
    "                self.hpt.report_hyperparameter_tuning_metric(\n",
    "                    hyperparameter_metric_tag=my_key,\n",
    "                    metric_value=my_value,\n",
    "                    global_step=self.global_step)\n",
    "                \n",
    "        if external_metrics is not None:\n",
    "            res_str = '|'\n",
    "            for my_key, my_value in external_metrics.items():\n",
    "                self.log(my_key, my_value)\n",
    "                res_str += ' {}: {:.5f} |'.format(my_key, my_value)\n",
    "                self.hpt.report_hyperparameter_tuning_metric(\n",
    "                    hyperparameter_metric_tag=my_key,\n",
    "                    metric_value=my_value,\n",
    "                    global_step=self.global_step)\n",
    "            border = '-'*len(res_str)\n",
    "            print(\"\\n\".join(['',border, res_str, border,'']))\n",
    "        \n",
    "        return None\n",
    "\n",
    "    #############\n",
    "    # PTL hooks #\n",
    "    #############\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        self.hpt = hypertune.HyperTune()\n",
    "        params = [ x for x in self.parameters() if x.requires_grad ]\n",
    "        print(f'Found {sum(p.numel() for p in params)} parameters')\n",
    "        optim_class = getattr(torch.optim,self.optimizer)\n",
    "        my_optimizer= optim_class(self.parameters(), lr=self.lr, \n",
    "                                  weight_decay=self.weight_decay, \n",
    "                                  betas=(self.beta1, self.beta2), \n",
    "                                  eps = self.eps,\n",
    "                                  amsgrad=self.amsgrad) \n",
    "        if self.scheduler is not None:\n",
    "            sch_dict = {\n",
    "                'scheduler': getattr(torch.optim.lr_scheduler,\n",
    "                                     self.scheduler)(my_optimizer, \n",
    "                                                     T_0=self.T_0, \n",
    "                                                     T_mult=self.T_mult, \n",
    "                                                     eta_min=self.eta_min,\n",
    "                                                     last_epoch = self.last_epoch), \n",
    "                'interval': self.scheduler_interval, \n",
    "                'name': 'learning_rate'\n",
    "            }\n",
    "            if self.scheduler_monitor is not None:\n",
    "                sch_dict['monitor'] = self.scheduler_monitor\n",
    "            return [my_optimizer], [sch_dict]\n",
    "        else:\n",
    "            return my_optimizer\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y   = batch\n",
    "        y_hat  = self(x)\n",
    "        loss   = self.criterion(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "       \n",
    "        x, y   = batch\n",
    "        y_hat = self(x)\n",
    "        loss   = self.criterion(y_hat, y)\n",
    "        self.log('valid_loss', loss)\n",
    "        metric = self.categorical_mse(y_hat, y)\n",
    "        \n",
    "        # 2. Add the outputs to the list\n",
    "        self.arit_means.append(loss)\n",
    "        self.harm_means.append(metric)\n",
    "        self.epoch_pred.append(y_hat)\n",
    "        self.epoch_label.append(y)\n",
    "        \n",
    "        return {'loss': loss, 'metric': metric, 'preds': y_hat, 'labels': y}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        arit_mean = torch.stack([ batch['loss'] for batch in val_step_outputs ], dim=0).mean()\n",
    "        harm_mean = torch.stack([ batch['metric'] for batch in val_step_outputs ], dim=0).mean(dim=0).pow(-1).mean().pow(-1)\n",
    "        epoch_preds = torch.cat([batch['preds'] for batch in val_step_outputs], dim=0)\n",
    "        epoch_labels  = torch.cat([batch['labels'] for batch in val_step_outputs], dim=0)\n",
    "        '''\n",
    "        # 3. Do something with all outputs\n",
    "        arit_mean = torch.stack( self.arit_means, dim=0).mean()\n",
    "        harm_mean = torch.stack(self.harm_means, dim=0).mean(dim=0).pow(-1).mean().pow(-1)\n",
    "        epoch_preds = torch.cat(self.epoch_pred, dim=0)\n",
    "        epoch_labels  = torch.cat(self.epoch_label, dim=0)\n",
    "        pearson, mean_pearson = pearson_correlation(epoch_preds, epoch_labels)\n",
    "        spearman, mean_spearman = spearman_correlation(epoch_preds, epoch_labels)\n",
    "        shannon_pred, shannon_label = shannon_entropy(epoch_preds), shannon_entropy(epoch_labels)\n",
    "        specificity_spearman, specificity_mean_spearman = spearman_correlation(shannon_pred, shannon_label)\n",
    "        self.aug_log(external_metrics={\n",
    "            'current_epoch': self.current_epoch, \n",
    "            'arithmetic_mean_loss': arit_mean,\n",
    "            'harmonic_mean_loss': harm_mean,\n",
    "            'prediction_mean_pearson': mean_pearson.item(),\n",
    "            'prediction_mean_spearman': mean_spearman.item(),\n",
    "            'entropy_spearman': specificity_mean_spearman.item()\n",
    "        })\n",
    "\n",
    "        return None\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "       \n",
    "        x, y = batch\n",
    "        y_pred = self(x)\n",
    "        loss = self.criterion(y_pred, y)\n",
    "        self.log('test_loss', loss)  \n",
    "        pearson, mean_pearson = pearson_correlation(y_pred, y)\n",
    "        self.log(\"test_pearson\", mean_pearson)\n",
    "        \n",
    "class CNNTransferLearning(CNNBasicTraining):\n",
    "    \n",
    "    ####################\n",
    "    # Standard methods #\n",
    "    ####################\n",
    "    \n",
    "    def __init__(self, model, parent_weights, frozen_epochs=0, \n",
    "                 optimizer='Adam', amsgrad = True,\n",
    "                          lr = 0.0032658700881052086,\n",
    "                          eps = 1e-08,\n",
    "                          weight_decay = 0.0003438210249762151,\n",
    "                          beta1 = 0.8661062881299633,\n",
    "                          beta2 = 0.879223105336538, scheduler=None, \n",
    "                 scheduler_monitor=None, scheduler_interval='epoch', \n",
    "                 #optimizer_args=None, scheduler_args=None\n",
    "                T_0=4096, T_mult=1, eta_min=0.0, last_epoch=-1):\n",
    "        \n",
    "        super().__init__(model, optimizer, amsgrad, lr, eps, weight_decay, beta1, beta2,\n",
    "                         scheduler, scheduler_monitor, \n",
    "                         scheduler_interval, \n",
    "                         #optimizer_args, scheduler_args,\n",
    "                         T_0, T_mult, eta_min, last_epoch)\n",
    "        \n",
    "        self.parent_weights = parent_weights\n",
    "        self.frozen_epochs  = frozen_epochs\n",
    "        \n",
    "    ###################\n",
    "    # Non-PTL methods #\n",
    "    ###################\n",
    "        '''\n",
    "    def attach_parent_weights(self, my_weights):\n",
    "\n",
    "        parent_state_dict = torch.load(my_weights)\n",
    "        if 'model_state_dict' in parent_state_dict.keys():\n",
    "            parent_state_dict = parent_state_dict['model_state_dict']\n",
    "            \n",
    "        mod_state_dict = filter_state_dict(self.model, parent_state_dict)\n",
    "        self.model.load_state_dict( mod_state_dict['filtered_state_dict'], strict=False )\n",
    "        return mod_state_dict['passed_keys']\n",
    "    \n",
    "    #############\n",
    "    # PTL hooks #\n",
    "    #############\n",
    "        \n",
    "    def setup(self, stage='training'):\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            if 'tar.gz' in self.parent_weights:\n",
    "                utils.unpack_artifact(self.parent_weights, tmpdirname)\n",
    "                old_model = utils.model_fn(os.path.join( tmpdirname, 'artifacts' ))\n",
    "                the_weights = os.path.join( tmpdirname, 'stash_dict.pkl' )\n",
    "                torch.save(old_model.state_dict(), the_weights)\n",
    "            elif 'gs://' in self.parent_weights:\n",
    "                subprocess.call(['gsutil','cp',self.parent_weights,tmpdirname])\n",
    "                the_weights = os.path.join( tmpdirname, os.path.basename(self.parent_weights) )\n",
    "            else:\n",
    "                the_weights = self.parent_weights\n",
    "            self.transferred_keys = self.attach_parent_weights(the_weights)\n",
    "        \n",
    "    def on_train_epoch_start(self):\n",
    "        print(f'starting epoch {self.current_epoch}')\n",
    "        for name, p in self.named_parameters():\n",
    "            if self.current_epoch < self.frozen_epochs:\n",
    "                if name in self.transferred_keys:\n",
    "                    p.requires_grad = False\n",
    "                else:\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                p.requires_grad = True\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3ed9c5-47a1-4fba-a100-a3618658c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "train_transform = t.Compose([\n",
    "    t.AddFlanks(left_flank, right_flank),\n",
    "    t.CenterCrop(600),\n",
    "    #t.Reverse(0.5),\n",
    "    t.Seq2Tensor()\n",
    "])\n",
    "val_test_transform = t.Compose([\n",
    "    t.AddFlanks(left_flank, right_flank),\n",
    "    t.CenterCrop(600),\n",
    "    t.Seq2Tensor()\n",
    "])\n",
    "\n",
    "target_transform = t_t.Compose([\n",
    "    t_t.Normalize(mean = 0.500, std = 1.059) # original for Malinois \n",
    "])\n",
    "activity_columns = ['HepG2','SKNSH', \"K562\"]\n",
    "stderr = ['lfcSE_k562', 'lfcSE_hepg2', 'lfcSE_sknsh']\n",
    "seq = \"nt_sequence\"\n",
    "# load the data\n",
    "train_dataset = MalinoisDataset(activity_columns = activity_columns, \n",
    "                              split = [1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 22, \"Y\"], \n",
    "                              filtration = \"original\",\n",
    "                                duplication_cutoff = 0.5,\n",
    "                                use_reverse_complement = True,\n",
    "                                stderr_columns = stderr,\n",
    "                                sequence_column = seq,\n",
    "                              transform = train_transform,\n",
    "                               target_transform = target_transform) \n",
    "\n",
    "val_dataset = MalinoisDataset(activity_columns = activity_columns, \n",
    "                              split = [7, 13], \n",
    "                              filtration = \"original\",\n",
    "                              sequence_column = seq,\n",
    "                              stderr_columns = stderr,\n",
    "                              transform = val_test_transform,\n",
    "                             target_transform = target_transform) \n",
    "\n",
    "test_dataset = MalinoisDataset(activity_columns = activity_columns, \n",
    "                              split = [9, 21, \"X\"],\n",
    "                              filtration = \"original\",\n",
    "                               sequence_column = seq,\n",
    "                               stderr_columns = stderr,\n",
    "                              transform = val_test_transform,\n",
    "                              target_transform = target_transform)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee079a1b-108d-4e31-9ce4-8ac4dc1e6992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MalinoisDataset of size 1691500 (MpraDaraset)\n",
      "    Number of datapoints: 1691500\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['1', '2', '3', '4', '5', '6', '8', '10', '11', '12', '14', '15', '16', '17', '18', '19', '20', '22', 'Y']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['HepG2_mean', 'SKNSH_mean', 'K562_mean']\n",
      "    Target columns that can be used: {'SKNSH_log2FC', 'K562_log2FC', 'HepG2_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n",
      "==================================================\n",
      "Dataset MalinoisDataset of size 55841 (MpraDaraset)\n",
      "    Number of datapoints: 55841\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['7', '13']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['HepG2_mean', 'SKNSH_mean', 'K562_mean']\n",
      "    Target columns that can be used: {'SKNSH_log2FC', 'K562_log2FC', 'HepG2_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n",
      "==================================================\n",
      "Dataset MalinoisDataset of size 45330 (MpraDaraset)\n",
      "    Number of datapoints: 45330\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['9', '21', 'X']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['HepG2_mean', 'SKNSH_mean', 'K562_mean']\n",
      "    Target columns that can be used: {'SKNSH_log2FC', 'K562_log2FC', 'HepG2_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"=\"*50)\n",
    "print(val_dataset)\n",
    "print(\"=\"*50)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a565107-f68f-4d27-9257-2d5c0387dc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name      | Type           | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model     | BassetBranched | 4.1 M  | train\n",
      "1 | criterion | L1KLmixed      | 0      | train\n",
      "-----------------------------------------------------\n",
      "4.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.1 M     Total params\n",
      "16.429    Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4107183 parameters\n",
      "Sanity Checking DataLoader 0: 100%|███████████████████████████████████████████████████████| 2/2 [00:00<00:00, 49.17it/s]\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.11787 | harmonic_mean_loss: 0.72142 | prediction_mean_pearson: -0.02437 | prediction_mean_spearman: -0.02319 | entropy_spearman: 0.01804 |\n",
      "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Epoch 0: 100%|████████████████████████████████████████████████████████████| 1573/1573 [02:41<00:00,  9.74it/s, v_num=43]\n",
      "Validation: |                                                                                     | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████████████████████████████████████████████▊  | 50/52 [00:01<00:00, 27.83it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████████████████████████████████████████████| 52/52 [00:01<00:00, 27.97it/s]\u001b[A\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | arithmetic_mean_loss: 0.11532 | harmonic_mean_loss: 0.65345 | prediction_mean_pearson: 0.62756 | prediction_mean_spearman: 0.52634 | entropy_spearman: 0.27552 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 1: 100%|████████████████████████████████████████████████████████████| 1573/1573 [02:59<00:00,  8.77it/s, v_num=43]\u001b[A\n",
      "Validation: |                                                                                     | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████████████████████████████████████████████▊  | 50/52 [00:02<00:00, 24.43it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████████████████████████████████████████████| 52/52 [00:02<00:00, 24.57it/s]\u001b[A\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | arithmetic_mean_loss: 0.10412 | harmonic_mean_loss: 0.57240 | prediction_mean_pearson: 0.66579 | prediction_mean_spearman: 0.55384 | entropy_spearman: 0.32042 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████| 1573/1573 [02:42<00:00,  9.68it/s, v_num=43]\u001b[A\n",
      "Validation: |                                                                                     | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                   | 0/52 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|███████████████████████████████████████████████████████▊  | 50/52 [00:01<00:00, 28.86it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████████████████████████████████████████████████████| 52/52 [00:01<00:00, 29.11it/s]\u001b[A\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | arithmetic_mean_loss: 0.10034 | harmonic_mean_loss: 0.57307 | prediction_mean_pearson: 0.65042 | prediction_mean_spearman: 0.51905 | entropy_spearman: 0.32604 |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████████████| 1573/1573 [02:45<00:00,  9.50it/s, v_num=43]\u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|████████████████████████████████████████████████████████████| 1573/1573 [02:45<00:00,  9.48it/s, v_num=43]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|█████████████████████████████████████████████████████████████| 43/43 [00:01<00:00, 26.73it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.09535475820302963\n",
      "      test_pearson          0.7158501148223877\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.09535475820302963, 'test_pearson': 0.7158501148223877}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BassetBranched(linear_dropout_p=0.11625456877954289,\n",
    "                      branched_activation='ReLU', branched_channels=140,\n",
    "                      branched_dropout_p=0.5757068086404574, \n",
    "                       n_outputs=len(train_dataset[0][1]),\n",
    "                      n_linear_layers=1, n_branched_layers=3, use_batch_norm=True,\n",
    "                      loss_criterion=L1KLmixed(beta=5.0, reduction='mean'))\n",
    "seq_model = CNNTransferLearning(model = model,\n",
    "    parent_weights = \"gs://tewhey-public-data/CODA_resources/my-model.epoch_5-step_19885.pkl\",\n",
    "                       frozen_epochs = 0,\n",
    "                          optimizer = \"Adam\",\n",
    "                          scheduler=\"CosineAnnealingWarmRestarts\",\n",
    "                          scheduler_interval=\"step\")\n",
    "\n",
    "callback_topmodel = pl.callbacks.ModelCheckpoint(monitor=\"prediction_mean_pearson\",\n",
    "                                                 save_top_k=1,\n",
    "                                                 dirpath=\"./Malinois\",\n",
    "                                                 filename=\"Malinois_max\")\n",
    "callback_es = pl.callbacks.early_stopping.EarlyStopping(monitor='prediction_mean_pearson', \n",
    "                                                        patience=30)\n",
    "\n",
    "#logger = pl_loggers.TensorBoardLogger(\"./logs\", name = \"Malinois_example\")\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu', \n",
    "    devices=[0], \n",
    "    min_epochs=1, \n",
    "    max_epochs=3,\n",
    "    precision=16,\n",
    "    log_every_n_steps = 0,\n",
    "    callbacks=[\n",
    "        #TQDMProgressBar(refresh_rate=50), \n",
    "        callback_es, \n",
    "        #callback_topmodel\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bc1a9-30e3-4954-9318-ae92b88c725d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mpra]",
   "language": "python",
   "name": "conda-env-mpra-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
