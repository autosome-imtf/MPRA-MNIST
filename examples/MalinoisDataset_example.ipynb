{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a9a8e6a-f156-47cd-ac97-5e2859b41671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import mpramnist\n",
    "from mpramnist.malinoisdataset import MalinoisDataset\n",
    "\n",
    "import mpramnist.transforms as t\n",
    "import mpramnist.target_transforms as t_t\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchsummary import summary\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b4112e-6aa2-46cf-931c-3b514a1764a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_flank = MalinoisDataset.LEFT_FLANK\n",
    "right_flank = MalinoisDataset.RIGHT_FLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "557a30c7-88c1-4384-b2f8-a5b1dd6950f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1024\n",
    "lr = 0.0033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86d47f5-30a9-413e-bfc1-1f2e68d12dee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "transform = t.Compose([\n",
    "    t.AddFlanks(left_flank, right_flank),\n",
    "    t.CenterCrop(600),\n",
    "    t.Seq2Tensor(),\n",
    "])\n",
    "\n",
    "target_transform = t_t.Compose([\n",
    "    t_t.Normalize(mean = 0.500, std = 1.059) # original for Malinois \n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = MalinoisDataset(activity_columns = ['HepG2','SKNSH'], \n",
    "                              split = \"train\", \n",
    "                              filtration = \"original\",\n",
    "                              transform = transform,\n",
    "                               target_transform = target_transform) \n",
    "val_dataset = MalinoisDataset(activity_columns = ['HepG2','SKNSH'], \n",
    "                              split =\"val\", \n",
    "                              filtration = \"original\",\n",
    "                              transform = transform,\n",
    "                             target_transform = target_transform) \n",
    "test_dataset = MalinoisDataset(activity_columns = ['HepG2','SKNSH'], \n",
    "                              split = \"test\",\n",
    "                              filtration = \"original\",\n",
    "                              transform = transform,\n",
    "                              target_transform = target_transform)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a482d7c-9df2-490b-a7c8-6660dca7d3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MalinoisDataset of size 627660 (MpraDaraset)\n",
      "    Number of datapoints: 627660\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['1', '2', '3', '4', '5', '6', '8', '9', '10', '11', '12', '14', '15', '16', '17', '18', '20', '22', 'Y']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['HepG2_log2FC', 'SKNSH_log2FC']\n",
      "    Target columns that can be used: {'K562_log2FC', 'HepG2_log2FC', 'SKNSH_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n",
      "===================\n",
      "Dataset MalinoisDataset of size 62582 (MpraDaraset)\n",
      "    Number of datapoints: 62582\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['7', '13']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['HepG2_log2FC', 'SKNSH_log2FC']\n",
      "    Target columns that can be used: {'K562_log2FC', 'HepG2_log2FC', 'SKNSH_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"===================\")\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bb49ad-0ba2-48f5-90b0-826581518c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b746f-e6af-4fa7-9c1f-3b4d598b0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basset(nn.Module):\n",
    "    def __init__(self, output_dim = 280):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1_channels=1000\n",
    "        self.linear2_channels=1000, \n",
    "\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "        # Layer 2 (convolutional), constituent parts\n",
    "        self.conv1_filters = torch.nn.Parameter(torch.zeros(300, 4, 19))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv1_filters)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(300)\n",
    "        self.maxpool1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # Layer 3 (convolutional), constituent parts\n",
    "        self.conv2_filters = torch.nn.Parameter(torch.zeros(200, 300, 11))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv2_filters)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(200)\n",
    "        self.maxpool2 = nn.MaxPool1d(4)\n",
    "\n",
    "        # Layer 4 (convolutional), constituent parts\n",
    "        self.conv3_filters = torch.nn.Parameter(torch.zeros(200, 200, 7))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv3_filters)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.maxpool3 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Layer 5 (fully connected), constituent parts\n",
    "        self.fc4 = nn.LazyLinear(1000, bias=True)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # Layer 6 (fully connected), constituent parts\n",
    "        self.fc5 = nn.LazyLinear(1000, bias=True)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        # Output layer (fully connected), constituent parts\n",
    "        self.fc6 = nn.LazyLinear(output_dim, bias=True)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Layer 1\n",
    "        cnn = torch.conv1d(x, self.conv1_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm1(cnn)\n",
    "        cnn = self.activation1(cnn)\n",
    "        cnn = self.maxpool1(cnn)\n",
    "\n",
    "        # Layer 2\n",
    "        cnn = torch.conv1d(cnn, self.conv2_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm2(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool2(cnn)\n",
    "\n",
    "        # Layer 3\n",
    "        cnn = torch.conv1d(cnn, self.conv3_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm3(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool3(cnn)\n",
    "\n",
    "        x = self.flatten(cnn)\n",
    "        return x\n",
    "\n",
    "    def decode(self, x):\n",
    "        # Layer 4\n",
    "        \n",
    "        cnn = self.fc4(x)\n",
    "        cnn = self.batchnorm4(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.dropout1(cnn)\n",
    "        \n",
    "        # Layer 5\n",
    "        cnn = self.fc5(cnn)\n",
    "        cnn = self.batchnorm5(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        x = self.dropout2(cnn)\n",
    "        return x\n",
    "    def classify(self, x):\n",
    "        \n",
    "        output = self.fc6(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        # Output layer\n",
    "        logits = self.fc6(cnn)\n",
    "        y_pred = self.output_activation(logits)\n",
    "\n",
    "        return y_pred\n",
    "        '''\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded)\n",
    "        output  = self.classify(decoded)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a55df33-548e-456c-80a4-8275a6e1842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shannon_entropy(x):\n",
    "    p_c = nn.Softmax(dim=1)(x)    \n",
    "    return torch.sum(- p_c * torch.log(p_c), axis=1)\n",
    "def pearson_correlation(x, y):\n",
    "    vx = x - torch.mean(x, dim=0)\n",
    "    vy = y - torch.mean(y, dim=0)\n",
    "    pearsons = torch.sum(vx * vy, dim=0) / (torch.sqrt(torch.sum(vx ** 2, dim=0)) * torch.sqrt(torch.sum(vy ** 2, dim=0)) + 1e-10)\n",
    "    return pearsons, torch.mean(pearsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f691ff24-baa0-4dd8-a5ad-70ce88f8b0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import PearsonCorrCoef\n",
    "class MPRA_Basset(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 output_dim = 3,\n",
    "                 learning_rate=1e-4,\n",
    "                 optimizer='Adam',\n",
    "                 scheduler=False,\n",
    "                 weight_decay=1e-6,\n",
    "                 epochs=1,\n",
    "                 extra_hidden_size = 100,\n",
    "                 criterion = 'MSELoss',\n",
    "                 last_activation='Tanh',\n",
    "                 sneaky_factor=1,\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.weight_decay = weight_decay\n",
    "        self.epochs = epochs\n",
    "        self.extra_hidden_size = extra_hidden_size\n",
    "        self.sneaky_factor = sneaky_factor\n",
    "        \n",
    "        self.criterion = getattr(nn, criterion)()  \n",
    "        self.last_activation = getattr(nn, last_activation)()\n",
    "        \n",
    "        self.basset_net = Basset()\n",
    "        \n",
    "        \n",
    "        self.basset_last_hidden_width = self.basset_net.linear2_channels\n",
    "\n",
    "        self.output_1 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width[0], self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )\n",
    "        \n",
    "        self.output_2 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width[0], self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )\n",
    "        \n",
    "        self.output_3 = nn.Sequential(\n",
    "            nn.Linear(self.basset_last_hidden_width[0], self.extra_hidden_size),\n",
    "            self.last_activation,\n",
    "            nn.Linear(self.extra_hidden_size, 1)\n",
    "            )       \n",
    "        self.val_pearson = PearsonCorrCoef()\n",
    "        self.example_input_array = torch.rand(1, 4, 600)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        basset_last_hidden = self.basset_net.decode(self.basset_net.encode(x))\n",
    "        output_1 = self.output_1(basset_last_hidden)\n",
    "        output_2 = self.output_2(basset_last_hidden)\n",
    "        output_3 = self.output_3(basset_last_hidden)\n",
    "        if self.output_dim == 2:\n",
    "            output_1 = torch.cat((output_1, output_2), dim=1)\n",
    "        elif self.output_dim == 3:\n",
    "            output_1 = torch.cat((output_1, output_2, output_3), dim=1)\n",
    "        return output_1\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs.to(device).float())\n",
    "\n",
    "        targets = targets.squeeze(1).to(device)\n",
    "        \n",
    "        shannon_pred, shannon_target = shannon_entropy(outputs).to(device), shannon_entropy(targets).to(device)\n",
    "        loss = self.criterion(outputs, \n",
    "                              targets) + self.sneaky_factor*self.criterion(shannon_pred, shannon_target)\n",
    "        self.log('train_loss', loss, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs.to(device).float())\n",
    "\n",
    "        targets = targets.squeeze(1).to(device)\n",
    "        \n",
    "        loss = self.criterion(outputs, \n",
    "                              targets)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        corr = self.val_pearson(outputs[:, 0], \n",
    "                              targets[:, 0])\n",
    "        self.log(\"val_pearson\", corr, on_epoch=True, prog_bar=True)\n",
    "        return {'loss': loss, 'pred': outputs, 'target': targets}\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        inputs, targets = batch\n",
    "        outputs = self(inputs.to(device).float())\n",
    "\n",
    "        targets = targets.squeeze(1).to(device)\n",
    "        \n",
    "        loss = self.criterion(outputs, \n",
    "                              targets)\n",
    "        self.log('test_loss', loss)\n",
    "        corr = self.val_pearson(outputs[:, 0], \n",
    "                              targets[:, 0])\n",
    "        self.log(\"test_pearson\", \n",
    "                 corr ,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 on_step=False,)\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            x, _ = batch\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(torch.optim, self.optimizer)(self.parameters(), lr=self.learning_rate,\n",
    "                                                         weight_decay=self.weight_decay)  \n",
    "        if self.scheduler:\n",
    "            lr_scheduler = {\n",
    "                'scheduler' : torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.epochs, eta_min=1e-6),\n",
    "                'name': 'learning_rate'\n",
    "                           }\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90155c55-0ed4-41b4-9cb8-4a951cb0b2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/lightning/pytorch/utilities/model_summary/model_summary.py:477: The total number of parameters detected may be inaccurate because the model contains an instance of `UninitializedParameter`. To get an accurate number, set `self.example_input_array` in your LightningModule.\n",
      "\n",
      "  | Name            | Type            | Params | Mode  | In sizes  | Out sizes\n",
      "------------------------------------------------------------------------------------\n",
      "0 | criterion       | MSELoss         | 0      | train | ?         | ?        \n",
      "1 | last_activation | Tanh            | 0      | train | [1, 100]  | [1, 100] \n",
      "2 | basset_net      | Basset          | 7.0 M  | train | ?         | ?        \n",
      "3 | output_1        | Sequential      | 100 K  | train | [1, 1000] | [1, 1]   \n",
      "4 | output_2        | Sequential      | 100 K  | train | [1, 1000] | [1, 1]   \n",
      "5 | output_3        | Sequential      | 100 K  | train | [1, 1000] | [1, 1]   \n",
      "6 | val_pearson     | PearsonCorrCoef | 0      | train | ?         | ?        \n",
      "------------------------------------------------------------------------------------\n",
      "7.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.3 M     Total params\n",
      "29.083    Total estimated model params size (MB)\n",
      "30        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                               | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=103` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                  | 0/613 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "seq_model = MPRA_Basset(output_dim = len(train_dataset[0][1]))\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    max_epochs=NUM_EPOCHS,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    callbacks=[TQDMProgressBar(refresh_rate=50)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bf1d71-771d-4b09-9813-b22b0288369e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mpra]",
   "language": "python",
   "name": "conda-env-mpra-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
