{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6376937-da7d-445b-a691-e701dd478b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mpramnist\n",
    "from mpramnist.malinoisdataset import MalinoisDataset\n",
    "from mpramnist.vikramdataset import VikramDataset\n",
    "from mpramnist import transforms as t\n",
    "from mpramnist import target_transforms as t_t\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb1d805-3499-48cc-83ac-d01f6593f6fd",
   "metadata": {},
   "source": [
    "# Initiate some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27bca7cc-41f9-4f00-b62b-ce25633bb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_flank = MalinoisDataset.LEFT_FLANK\n",
    "right_flank = MalinoisDataset.RIGHT_FLANK\n",
    "BATCH_SIZE = 1076\n",
    "NUM_WORKERS = 103\n",
    "\n",
    "# default parametes\n",
    "activity_columns = ['HepG2','SKNSH', \"K562\"]\n",
    "stderr = ['K562_lfcSE', 'HepG2_lfcSE', 'SKNSH_lfcSE']\n",
    "seq = \"sequence\"\n",
    "stderr_threshold = 1.0,\n",
    "std_multiple_cut = 6.0,\n",
    "up_cutoff_move = 3.0,\n",
    "duplication_cutoff = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8faf383-22ac-4714-9583-423dd7fff0b2",
   "metadata": {},
   "source": [
    "# You wanna test original parametes?\n",
    "\n",
    "So use this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "835eb05d-b12a-41c4-bfa4-d97d6d9e3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MalinoisDataset(split = \"train\",\n",
    "                                filtration = \"original\", # use \"original for author's parameters\"\n",
    "                                duplication_cutoff = 0.5,\n",
    "                                use_original_reverse_complement = True # this parameter paddes sequences and does rev comp\n",
    "                               )\n",
    "\n",
    "val_dataset = MalinoisDataset(split = \"val\",\n",
    "                              filtration = \"original\") \n",
    "\n",
    "test_dataset = MalinoisDataset(split = \"test\",\n",
    "                              filtration = \"original\")\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb60bfc9-e6e4-463e-9629-761cfafb491e",
   "metadata": {},
   "source": [
    "# Use your own parameters\n",
    "\n",
    "Use this part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e341a5-4a73-4bf8-9869-51b37723b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "train_transform = t.Compose([\n",
    "    t.AddFlanks(left_flank, right_flank),\n",
    "    t.CenterCrop(600),\n",
    "    t.Reverse(0.5),\n",
    "    t.Seq2Tensor()\n",
    "])\n",
    "val_test_transform = t.Compose([\n",
    "    t.AddFlanks(left_flank, right_flank),\n",
    "    t.CenterCrop(600),\n",
    "    t.Seq2Tensor()\n",
    "])\n",
    "\n",
    "# load the data\n",
    "train_dataset = MalinoisDataset( \n",
    "                              split = \"train\", \n",
    "                              transform = train_transform,\n",
    "                              filtration = \"own\",\n",
    "                              duplication_cutoff = 0.5) \n",
    "val_dataset = MalinoisDataset(\n",
    "                              split = \"val\",\n",
    "                              filtration = \"own\",\n",
    "                              transform = val_test_transform) \n",
    "test_dataset = MalinoisDataset(\n",
    "                              split = \"test\", \n",
    "                              filtration = \"own\",\n",
    "                              transform = val_test_transform)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = NUM_WORKERS)\n",
    "val_loader = data.DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)\n",
    "test_loader = data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers = NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef92c7-e827-4568-bc15-6f73360627c5",
   "metadata": {},
   "source": [
    "# Some dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51225397-16f5-4e91-93b0-df40f1708597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MalinoisDataset of size 932088 (MpraDaraset)\n",
      "    Number of datapoints: 932088\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['1', '2', '3', '4', '5', '6', '8', '9', '10', '11', '12', '14', '15', '16', '17', '18', '20', '22', 'Y']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['K562_log2FC', 'HepG2_log2FC', 'SKNSH_log2FC']\n",
      "    Target columns that can be used: {'HepG2_log2FC', 'SKNSH_log2FC', 'K562_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n",
      "==================================================\n",
      "Dataset MalinoisDataset of size 58809 (MpraDaraset)\n",
      "    Number of datapoints: 58809\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['19', '21', 'X']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['K562_log2FC', 'HepG2_log2FC', 'SKNSH_log2FC']\n",
      "    Target columns that can be used: {'HepG2_log2FC', 'SKNSH_log2FC', 'K562_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n",
      "==================================================\n",
      "Dataset MalinoisDataset of size 62582 (MpraDaraset)\n",
      "    Number of datapoints: 62582\n",
      "    Default split folds: {'train': '1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 20, 22, Y', 'val': '19, 21, X', 'test': '7, 13'}\n",
      "    Used split fold: ['7', '13']\n",
      "    Scalar features: {}\n",
      "    Vector features: {}\n",
      "    Cell types: ['HepG2', 'K562', 'SKNSH']\n",
      "    Сell type used: ['K562_log2FC', 'HepG2_log2FC', 'SKNSH_log2FC']\n",
      "    Target columns that can be used: {'HepG2_log2FC', 'SKNSH_log2FC', 'K562_log2FC'}\n",
      "    Number of channels: 4\n",
      "    Sequence size: 600\n",
      "    Number of samples: {'train': 668946, 'val': 62406, 'test': 66712}\n",
      "    Description: MalinoisDataset is based on \n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(\"=\"*50)\n",
    "print(val_dataset)\n",
    "print(\"=\"*50)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e6e5a1-f7dc-4521-b1a8-645e8c0437b4",
   "metadata": {},
   "source": [
    "# Define loss criterion from Malinois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26c59939-0ae2-47c7-bb67-5be3b4ef8b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1KLmixed(nn.Module):\n",
    "    \n",
    "    def __init__(self, reduction='mean', alpha=1.0, beta=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.reduction = reduction\n",
    "        self.alpha = alpha\n",
    "        self.beta  = beta\n",
    "        \n",
    "        self.MSE = nn.L1Loss(reduction=reduction.replace('batch',''))\n",
    "        self.KL  = nn.KLDivLoss(reduction=reduction, log_target=True)\n",
    "        \n",
    "    def forward(self, preds, targets):\n",
    "        preds_log_prob  = preds   - torch.logsumexp(preds, dim=-1, keepdim=True)\n",
    "        target_log_prob = targets - torch.logsumexp(targets, dim=-1, keepdim=True)\n",
    "        \n",
    "        MSE_loss = self.MSE(preds, targets)\n",
    "        KL_loss  = self.KL(preds_log_prob, target_log_prob)\n",
    "        \n",
    "        combined_loss = MSE_loss.mul(self.alpha) + \\\n",
    "                        KL_loss.mul(self.beta)\n",
    "        \n",
    "        return combined_loss.div(self.alpha+self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4291656-f938-4ae7-955e-59f791b33379",
   "metadata": {},
   "source": [
    "# Define Basset model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cb8b070-c1a6-4e18-a788-c695889340f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basset_Net(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.output_activation = nn.Sigmoid()\n",
    "\n",
    "        # Layer 2 (convolutional), constituent parts\n",
    "        self.conv1_filters = torch.nn.Parameter(torch.zeros(300, 4, 19))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv1_filters)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(300)\n",
    "        self.maxpool1 = nn.MaxPool1d(3)\n",
    "\n",
    "        # Layer 3 (convolutional), constituent parts\n",
    "        self.conv2_filters = torch.nn.Parameter(torch.zeros(200, 300, 11))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv2_filters)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(200)\n",
    "        self.maxpool2 = nn.MaxPool1d(4)\n",
    "\n",
    "        # Layer 4 (convolutional), constituent parts\n",
    "        self.conv3_filters = torch.nn.Parameter(torch.zeros(200, 200, 7))\n",
    "        torch.nn.init.kaiming_uniform_(self.conv3_filters)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(200)\n",
    "        self.maxpool3 = nn.MaxPool1d(2)\n",
    "\n",
    "        # Layer 5 (fully connected), constituent parts\n",
    "        self.fc4 = nn.LazyLinear(1000, bias=True)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(1000)\n",
    "        \n",
    "        # Layer 6 (fully connected), constituent parts\n",
    "        self.fc5 = nn.LazyLinear(1000, bias=True)\n",
    "        self.batchnorm5 = nn.BatchNorm1d(1000)\n",
    "\n",
    "        # Output layer (fully connected), constituent parts\n",
    "        self.fc6 = nn.LazyLinear(output_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        # Layer 2\n",
    "        cnn = torch.conv1d(x, self.conv1_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm1(cnn)\n",
    "        cnn = self.activation1(cnn)\n",
    "        cnn = self.maxpool1(cnn)\n",
    "\n",
    "        # Layer 3\n",
    "        cnn = torch.conv1d(cnn, self.conv2_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm2(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool2(cnn)\n",
    "\n",
    "        # Layer 4\n",
    "        cnn = torch.conv1d(cnn, self.conv3_filters, stride=1, padding=\"same\")\n",
    "        cnn = self.batchnorm3(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.maxpool3(cnn)\n",
    "\n",
    "        # Layer 5\n",
    "        cnn = self.flatten(cnn)\n",
    "        cnn = self.fc4(cnn)\n",
    "        cnn = self.batchnorm4(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.dropout1(cnn)\n",
    "        \n",
    "        # Layer 6\n",
    "        cnn = self.fc5(cnn)\n",
    "        cnn = self.batchnorm5(cnn)\n",
    "        cnn = self.activation(cnn)\n",
    "        cnn = self.dropout2(cnn)\n",
    "\n",
    "        # Output layer\n",
    "        logits = self.fc6(cnn)\n",
    "        y_pred = self.output_activation(logits)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f6bc2-3205-4342-be23-1c400a5404c3",
   "metadata": {},
   "source": [
    "# Define Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c89de7d-52ee-46e5-b877-f19fc067dd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, output_dim = 3, seq_len = 600, in_ch = 4, block_sizes=[16, 24, 32, 40, 48, 56, 64], kernel_size=7):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = output_dim\n",
    "        out_ch = 64\n",
    "        nn_blocks = []\n",
    "      \n",
    "        for in_bs, out_bs in zip([in_ch] + block_sizes, block_sizes):\n",
    "            \n",
    "            block = nn.Sequential(\n",
    "                nn.Conv1d(in_bs, out_bs, kernel_size=kernel_size, padding=1),\n",
    "                nn.SiLU(),\n",
    "                nn.BatchNorm1d(out_bs)\n",
    "            )\n",
    "            nn_blocks.append(block)\n",
    "            \n",
    "        self.conv_net = nn.Sequential(\n",
    "            *nn_blocks,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(block_sizes[-1] * (seq_len + len(block_sizes)*(3-kernel_size)), out_ch),\n",
    "        )\n",
    "        self.head = nn.Sequential(nn.Linear(out_ch, out_ch),\n",
    "                                   nn.BatchNorm1d(out_ch),\n",
    "                                   nn.SiLU(),\n",
    "                                   nn.Linear(out_ch, self.out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "       \n",
    "        out = self.conv_net(x)\n",
    "        out = self.head(out)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07460818-0b47-4e75-8125-6b37c89e2e39",
   "metadata": {},
   "source": [
    "# Define metrics and SeqModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6aaa9021-a6d8-4394-8d1b-0d324b93a461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as L\n",
    "\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from torchmetrics import PearsonCorrCoef\n",
    "class Seq1Model(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, out_ch, lr=3e-4):\n",
    "        super().__init__()\n",
    "        self.model = Simple_Net(output_dim = out_ch)\n",
    "        #self.model = Basset_Net(output_dim = out_ch)\n",
    "        #self.loss = nn.MSELoss() \n",
    "        self.loss = L1KLmixed(beta=5.0, reduction='mean')\n",
    "        self.lr = lr\n",
    "        self.pearson = PearsonCorrCoef()\n",
    "        self.val_loss = []\n",
    "        self.val_pears = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        X, y = batch\n",
    "        y_hat = self.model(X)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True,  on_step=True, on_epoch=True, logger = True)\n",
    "        \n",
    "        lr = self.optimizers().param_groups[0]['lr']  # Get current learning rate\n",
    "        self.log('learning_rate', lr, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_loss.append(loss)\n",
    "\n",
    "        corr = self.pearson(y_hat[:,0], y[:,0])\n",
    "        self.val_pears.append(corr)\n",
    "        self.log(\"val_pearson\", corr, on_epoch=True, prog_bar=True, on_step = False)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "\n",
    "        val_loss = torch.stack(self.val_loss, dim = 0).mean()\n",
    "        val_pears = torch.stack(self.val_pears, dim = 0).mean()\n",
    "        \n",
    "        res_str = '|' + ' {}: {:.5f} |'.format(\"current_epoch\", self.current_epoch) \n",
    "        res_str += ' {}: {:.5f} |'.format(\"val_loss\", val_loss)\n",
    "        res_str += ' {}: {:.5f} |'.format(\"val_pearson\", val_pears)\n",
    "        border = '-'*len(res_str)\n",
    "        print(\"\\n\".join(['',border, res_str, border,'']))\n",
    "        self.val_loss.clear()\n",
    "        self.val_pears.clear()\n",
    "        return None\n",
    "        \n",
    "    def test_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        y_hat = self.model(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        \n",
    "        self.log('test_loss', \n",
    "                 loss, \n",
    "                 prog_bar=True, \n",
    "                 on_step=False,\n",
    "                 on_epoch=True)\n",
    "        corr = self.pearson(y_hat[:,0], y[:,0])\n",
    "        self.log(\"test_pearson\", \n",
    "                 corr ,\n",
    "                 on_epoch=True,\n",
    "                 prog_bar=True,\n",
    "                 on_step=False,)\n",
    "        \n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        if isinstance(batch, tuple) or isinstance(batch, list):\n",
    "            x, _ = batch\n",
    "        else:\n",
    "            x = batch\n",
    "        return self(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(self.parameters(),\n",
    "                                               lr=self.lr,\n",
    "                                               weight_decay = 0.01)\n",
    "        '''\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(),\n",
    "                                          betas = (0.8661062881299633,0.879223105336538),\n",
    "                                          weight_decay = 0.0003438210249762151,\n",
    "                                          lr = self.lr,\n",
    "                                          amsgrad = True)\n",
    "        '''\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(self.optimizer, # type: ignore\n",
    "                                                        max_lr=self.lr,\n",
    "                                                        three_phase=False, \n",
    "                                                        total_steps=self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                        pct_start=0.3,\n",
    "                                                        cycle_momentum =False)\n",
    "        lr_scheduler_config = {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"frequency\": 1,\n",
    "                    \"name\": \"cycle_lr\"\n",
    "            }\n",
    "            \n",
    "        '''\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer = self.optimizer,\n",
    "                                                                            T_0 = 4096,\n",
    "                                                                            T_mult=1,\n",
    "                                                                            eta_min=0.0,\n",
    "                                                                            last_epoch=-1)\n",
    "        lr_scheduler_config = {\n",
    "                    \"scheduler\": lr_scheduler,\n",
    "                    \"interval\": \"step\",\n",
    "                    \"name\": 'learning_rate'\n",
    "            }\n",
    "            \n",
    "            '''\n",
    "        return [self.optimizer], [lr_scheduler_config]\n",
    "        \n",
    "        #return self.optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453cfaa-e7e8-4e2f-97ad-31b77d4f9a18",
   "metadata": {},
   "source": [
    "# LR-finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec616995-66e9-44ac-9ea7-4d44b36a5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_lr_finder import LRFinder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = Simple_Net(output_dim = 3)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7, weight_decay = 0.01,)\n",
    "'''\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                          betas = (0.8661062881299633,0.879223105336538),\n",
    "                                          weight_decay = 0.0003438210249762151,\n",
    "                                          lr = 1e-7,\n",
    "                                          amsgrad = True)'''\n",
    "criterion = L1KLmixed(beta=5.0, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1d11a0d-e2ac-4038-9301-f4a9d7454f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "463afcdd-6ddf-4b29-93e6-7b38c9eb6759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b349ed0ab342c4be1028b57c20ef58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    }
   ],
   "source": [
    "lr_finder.range_test(train_loader, start_lr=1e-7, end_lr=10, num_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48c17859-5605-4fd6-9281-d56ee962ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR suggestion: steepest gradient\n",
      "Suggested LR: 7.05E-03\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder.plot()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ac6be-3630-4d7c-b49c-a49176b11a2e",
   "metadata": {},
   "source": [
    "# Start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61afc00-ea49-4c2a-b5c1-dd24e1162eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name    | Type            | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | model   | Simple_Net      | 1.8 M  | train\n",
      "1 | loss    | L1KLmixed       | 0      | train\n",
      "2 | pearson | PearsonCorrCoef | 0      | train\n",
      "----------------------------------------------------\n",
      "1.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.8 M     Total params\n",
      "7.271     Total estimated model params size (MB)\n",
      "33        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/home/nios/miniconda3/envs/mpra/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The variance of predictions or target is close to zero. This can cause instability in Pearson correlationcoefficient, leading to wrong results. Consider re-scaling the input if possible or computing using alarger dtype (currently using torch.float32).\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 0.14999 | val_pearson: 0.00000 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 0.00000 | val_loss: 0.14129 | val_pearson: 0.55383 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 1.00000 | val_loss: 0.12823 | val_pearson: 0.67179 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 2.00000 | val_loss: 0.11982 | val_pearson: 0.71774 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 3.00000 | val_loss: 0.11954 | val_pearson: 0.72345 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 4.00000 | val_loss: 0.12130 | val_pearson: 0.74753 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 5.00000 | val_loss: 0.11290 | val_pearson: 0.76465 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 6.00000 | val_loss: 0.11184 | val_pearson: 0.76662 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 7.00000 | val_loss: 0.11215 | val_pearson: 0.75762 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 8.00000 | val_loss: 0.10728 | val_pearson: 0.77761 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "| current_epoch: 9.00000 | val_loss: 0.10869 | val_pearson: 0.77818 |\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 10.00000 | val_loss: 0.11388 | val_pearson: 0.77717 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 11.00000 | val_loss: 0.10608 | val_pearson: 0.78908 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 12.00000 | val_loss: 0.11313 | val_pearson: 0.78317 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 13.00000 | val_loss: 0.10602 | val_pearson: 0.79133 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 14.00000 | val_loss: 0.10732 | val_pearson: 0.79310 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 15.00000 | val_loss: 0.10698 | val_pearson: 0.79901 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 16.00000 | val_loss: 0.10384 | val_pearson: 0.79557 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 17.00000 | val_loss: 0.10799 | val_pearson: 0.80510 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 18.00000 | val_loss: 0.10372 | val_pearson: 0.80882 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 19.00000 | val_loss: 0.10098 | val_pearson: 0.80796 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 20.00000 | val_loss: 0.09942 | val_pearson: 0.80879 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 21.00000 | val_loss: 0.10245 | val_pearson: 0.81267 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 22.00000 | val_loss: 0.09914 | val_pearson: 0.81342 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 23.00000 | val_loss: 0.09835 | val_pearson: 0.81595 |\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "| current_epoch: 24.00000 | val_loss: 0.09851 | val_pearson: 0.81603 |\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.08779573440551758\n",
      "      test_pearson          0.7577244639396667\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.08779573440551758, 'test_pearson': 0.7577244639396667}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_model = Seq1Model(out_ch=len(train_dataset[0][1]), lr = 4.75e-3)\n",
    "\n",
    "logger = pl_loggers.TensorBoardLogger(\"./logs_search_architecture\", name = \"Simple_with_OneCyclelr_L1KLmixed_lr4.75e-3\")\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[0],\n",
    "    max_epochs=25,\n",
    "    gradient_clip_val=1,\n",
    "    precision='16-mixed', \n",
    "    enable_progress_bar = False,\n",
    "    #callbacks=[TQDMProgressBar(refresh_rate=55)]\n",
    "    logger = logger\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(seq_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "trainer.test(seq_model, dataloaders=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e384ce-0399-447c-9caf-b0126a7dff7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mpra]",
   "language": "python",
   "name": "conda-env-mpra-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
